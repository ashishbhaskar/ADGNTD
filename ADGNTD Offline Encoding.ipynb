{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d618f3-fe60-4e2c-a68a-e0bf86ef04df",
   "metadata": {},
   "source": [
    "1. Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5b252b-b734-41f0-92cc-bb462fda2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import bmat, coo_matrix, csr_matrix, diags, kron, load_npz\n",
    "from scipy import linalg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, TimeDistributed, Flatten, Reshape\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tabulate import tabulate\n",
    "from itertools import chain\n",
    "from termcolor import colored\n",
    "from random import randrange\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp\n",
    "import time\n",
    "import traceback\n",
    "import datetime\n",
    "import math\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c481e9-f7d1-444b-a57d-aeaa8f496648",
   "metadata": {},
   "source": [
    "3. Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8266941f-43b5-45db-93e7-a71cdc6bbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. folding and Unfolding a 3D tensor\n",
    "def unfolding_3D(Tens, unfol_dim, other_dim_seq ):\n",
    "    X = Tens.transpose(unfol_dim, other_dim_seq[0], other_dim_seq[1]).reshape(Tens.shape[unfol_dim], Tens.shape[other_dim_seq[0]] * Tens.shape[other_dim_seq[1]])\n",
    "    return (X)\n",
    "def folding_3D(Unfold_Tens, unfol_dim , other_dim_seq, Tens_shape):\n",
    "    a = [0,1,2]\n",
    "    items = deque(a)\n",
    "    items.rotate(unfol_dim) \n",
    "    a_up= list(items)\n",
    "    X = Unfold_Tens.reshape(Tens_shape[unfol_dim], Tens_shape[other_dim_seq[0]],Tens_shape[other_dim_seq[1]]).transpose(a_up[0], a_up[1], a_up[2])    \n",
    "    return (X)\n",
    "#2. ADGNTD functions\n",
    "def derivative_dynamic_graph_A_W(A, B, W):\n",
    "    In = csr_matrix(np.identity(A.shape[1]))\n",
    "    Ip = csr_matrix(np.identity(B.shape[0]))\n",
    "    Imn = csr_matrix(np.identity(A.shape[0] * A.shape[1]))\n",
    "    ones = csr_matrix(np.ones((1, A.shape[0])))\n",
    "    Eta_rep = csr_matrix(linalg.khatri_rao(Imn.toarray(), (B.dot(kron(In, ones))).toarray()))\n",
    "    Eta = Eta_rep.T.dot(kron(In.T, W)).dot(Eta_rep)\n",
    "    derivative_A = (Eta + Eta.T).dot(vect(A))\n",
    "    derivative_A_mat = vec_to_Mat(derivative_A, A.shape)\n",
    "    return(csr_matrix(derivative_A_mat))\n",
    "def derivative_dynamic_graph_A_D(A, B, Deg):\n",
    "    In = csr_matrix(np.identity(A.shape[1]))\n",
    "    Ip = csr_matrix(np.identity(B.shape[0]))\n",
    "    Imn = csr_matrix(np.identity(A.shape[0] * A.shape[1]))\n",
    "    ones = csr_matrix(np.ones((1, A.shape[0])))\n",
    "    Eta_rep = csr_matrix(linalg.khatri_rao(Imn.toarray(), (B.dot(kron(In, ones))).toarray()))\n",
    "    Eta = Eta_rep.T.dot(kron(In.T, Deg)).dot(Eta_rep)\n",
    "    derivative_A = (Eta + Eta.T).dot(vect(A))\n",
    "    derivative_A_mat = vec_to_Mat(derivative_A, A.shape)\n",
    "    return(csr_matrix(derivative_A_mat))\n",
    "def derivative_dynamic_graph_B_W(A, B, W):\n",
    "    In = csr_matrix(np.identity(A.shape[1]))\n",
    "    Ip = csr_matrix(np.identity(B.shape[0]))\n",
    "    I_dash = diags(np.ones((W.shape[0],)))\n",
    "    Xi_rep = kron(linalg.khatri_rao(In.toarray(), A.toarray()), Ip)\n",
    "    Xi = Xi_rep.T.dot(kron(In.T , W)).dot(Xi_rep)\n",
    "    derivative_B = (Xi + Xi.T).dot(vect(B))\n",
    "    derivative_B_mat = vec_to_Mat(derivative_B, B.shape)\n",
    "    return(csr_matrix(derivative_B_mat))\n",
    "def derivative_dynamic_graph_B_D(A, B, Deg):\n",
    "    In = csr_matrix(np.identity(A.shape[1]))\n",
    "    Ip = csr_matrix(np.identity(B.shape[0]))\n",
    "    Xi_rep = kron(linalg.khatri_rao(In.toarray(), A.toarray()), Ip)\n",
    "    Xi = Xi_rep.T.dot(kron(In.T , Deg)).dot(Xi_rep)\n",
    "    derivative_B = (Xi + Xi.T).dot(vect(B))\n",
    "    derivative_B_mat = vec_to_Mat(derivative_B, B.shape)\n",
    "    return(csr_matrix(derivative_B_mat))\n",
    "def vect(Mat):\n",
    "    x = Mat.shape[0]\n",
    "    y = Mat.shape[1]\n",
    "    vec_A = Mat.T.reshape(x*y,1)\n",
    "    return(vec_A)\n",
    "def vec_to_Mat(vec, shape):\n",
    "    x = shape[0]\n",
    "    y = shape[1]\n",
    "    Mat = vec.reshape(y,x).T\n",
    "    return (Mat)\n",
    "def rmse(a, b):\n",
    "    rms = np.sqrt(np.power(a-b, 2).sum()/(a.shape[0]* a.shape[1]))\n",
    "    return (round(rms,2))\n",
    "    \n",
    "def calculate_smoothed_weights(mat):\n",
    "    identity_matrix = np.identity(mat.shape[0])\n",
    "    W_smooth = -pd.DataFrame(identity_matrix).rolling(2, axis=1).sum().replace(np.nan, 0)\n",
    "    W_smooth[0] = -pd.DataFrame(identity_matrix)[0]\n",
    "    W_smooth = np.array(W_smooth + 2 * pd.DataFrame(identity_matrix))\n",
    "    W_smooth_pos = (np.abs(W_smooth) + W_smooth) / 2\n",
    "    W_smooth_neg = (np.abs(W_smooth) - W_smooth) / 2\n",
    "    neum_term_B = csr_matrix(W_smooth_neg.T.dot(W_smooth_neg))\n",
    "    denom_term_B = csr_matrix(W_smooth_pos.T.dot(W_smooth_pos))\n",
    "    return  (neum_term_B, denom_term_B)\n",
    "\n",
    "def calculate_smoothed_weights_cp(mat):\n",
    "    identity_matrix = np.identity(mat.shape[0])\n",
    "    W_smooth = -pd.DataFrame(identity_matrix).rolling(2, axis=1).sum().replace(np.nan, 0)\n",
    "    W_smooth[0] = -pd.DataFrame(identity_matrix)[0]\n",
    "    W_smooth = np.array(W_smooth + 2 * pd.DataFrame(identity_matrix))\n",
    "    W_smooth_pos = (np.abs(W_smooth) + W_smooth) / 2\n",
    "    W_smooth_neg = (np.abs(W_smooth) - W_smooth) / 2\n",
    "    neum_term_B = cp.asarray(W_smooth_neg.T.dot(W_smooth_neg))\n",
    "    denom_term_B = cp.asarray(W_smooth_pos.T.dot(W_smooth_pos))\n",
    "    return  (neum_term_B, denom_term_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90092eb-c10f-43d9-9b51-55bc3db79b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADGNTD(Tens1, W_prior, r = 10, n_iter=100, hy1=1, hy2=1, mu=0.1, factorization1=1000,\n",
    "                      prin=50, A = None, B = None, C = None, W = None, delta = None ):\n",
    "    T3 = csr_matrix(unfolding_3D(Tens1, unfol_dim = 2, other_dim_seq = [0,1] ))\n",
    "    plot = []\n",
    "    try:\n",
    "        if (A == None) | (B == None)|  (C == None):\n",
    "            A = csr_matrix(np.random.rand(Tens1.shape[0],r))\n",
    "            B = csr_matrix(np.random.rand(Tens1.shape[1],r))\n",
    "            C = csr_matrix(np.random.rand(Tens1.shape[2],r))\n",
    "        if (W == None):\n",
    "            W = W_prior.copy()\n",
    "            delta = W*.5\n",
    "        if (delta == None):\n",
    "            delta = W*.5\n",
    "        A_list = list(range(A.shape[0]))\n",
    "        B_list = list(range(B.shape[0]))\n",
    "        C_list = list(range(C.shape[0]))\n",
    "        neum_term_B, denom_term_B = calculate_smoothed_weights(B)\n",
    "        for indomie in range(n_iter):\n",
    "            A_Batch = Tens1.shape[0] // 1\n",
    "            B_Batch = Tens1.shape[1] // 1\n",
    "            C_Batch = Tens1.shape[2] // 1\n",
    "            A_int = random.randrange(A.shape[0])\n",
    "            B_int = random.randrange(B.shape[0])\n",
    "            C_int = random.randrange(C.shape[0])\n",
    "            A_Sampling = random.sample(A_list, A_Batch)\n",
    "            B_Sampling = random.sample(B_list, B_Batch)\n",
    "            C_Sampling = random.sample(C_list, C_Batch)\n",
    "            A_Sampled = csr_matrix(A[A_Sampling, :])\n",
    "            B_Sampled = csr_matrix(B[B_Sampling, :])\n",
    "            C_Sampled = csr_matrix(C[C_Sampling, :])\n",
    "            Tens_sample = Tens1[A_Sampling, :, :][:, B_Sampling, :][:, :, C_Sampling]\n",
    "            T1_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=0, other_dim_seq=[1, 2]))\n",
    "            T2_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=1, other_dim_seq=[2, 0]))\n",
    "            T3_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=2, other_dim_seq=[0, 1]))\n",
    "            # Loop through B_Sampling indices and create w_list_B\n",
    "            for index, j in enumerate(B_Sampling):\n",
    "                if index == 0:\n",
    "                    w_list_B = list(range(j * A.shape[0], (j + 1) * A.shape[0]))\n",
    "                if index > 0:\n",
    "                    w_list_B += list(range(j * A.shape[0], (j + 1) * A.shape[0]))\n",
    "            \n",
    "            # Loop through B indices and create w_list_A\n",
    "            for i in range(0, B.shape[0]):\n",
    "                if i == 0:\n",
    "                    w_list_A = list(np.array(A_Sampling) + i * A.shape[0])\n",
    "                if i > 0:\n",
    "                    w_list_A += list(np.array(A_Sampling) + i * A.shape[0])\n",
    "            \n",
    "            # Find common indices between w_list_A and w_list_B\n",
    "            w_list = list(set(w_list_A) & set(w_list_B))\n",
    "            # Sample submatrix W_sampled from W using w_list indices\n",
    "            W_sampled = W[w_list, :][:, w_list]\n",
    "            # Calculate degree matrix for sampled nodes\n",
    "            Deg_sampled = sparse.diags(np.squeeze(np.asarray(W_sampled.sum(axis=0)))).tocsr()\n",
    "            # Sample submatrix W_prior_sampled from W_prior using w_list indices\n",
    "            W_prior_sampled = W_prior[w_list, :][:, w_list]\n",
    "            # Sample submatrix delta_sampled from delta using w_list indices\n",
    "            delta_sampled = delta[w_list, :][:, w_list]\n",
    "            # Create matrix V using Khatri-Rao product of B_Sampled and C_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(B_Sampled.toarray(), C_Sampled.toarray()))\n",
    "            # Update A_Sampled using optimization step\n",
    "            A_Sampled = csr_matrix(A_Sampled.multiply(T1_sampled.dot(V) \n",
    "                                                      + hy1 * derivative_dynamic_graph_A_W(A_Sampled, B_Sampled, W_sampled)) / \n",
    "                                   (A_Sampled.dot(V.T).dot(V) + hy2 * A_Sampled + \n",
    "                                    hy1 * derivative_dynamic_graph_A_D(A_Sampled, B_Sampled, Deg_sampled)))\n",
    "            # Update rows of A using A_Sampling indices\n",
    "            A[A_Sampling, :] = A_Sampled\n",
    "            # Recalculate matrix V using Khatri-Rao product of C_Sampled and A_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(C_Sampled.toarray(), A_Sampled.toarray()))\n",
    "            # Update B_Sampled using optimization step\n",
    "            B_num = B_Sampled.multiply(csr_matrix(T2_sampled.dot(V)) + 5 * (neum_term_B.dot(B)) \n",
    "                                       + hy1 * derivative_dynamic_graph_B_W(A_Sampled, B_Sampled, W_sampled))\n",
    "            B_denom = (B_Sampled.dot(V.T).dot(V) + hy2 * B_Sampled + 5 * (denom_term_B.dot(B)) \n",
    "                       + hy1 * derivative_dynamic_graph_B_D(A_Sampled, B_Sampled, Deg_sampled))\n",
    "            B_Sampled = csr_matrix(B_num / B_denom)\n",
    "            # Update rows of B using B_Sampling indices\n",
    "            B[B_Sampling, :] = B_Sampled\n",
    "            # Recalculate matrix V using Khatri-Rao product of A_Sampled and B_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(A_Sampled.toarray(), B_Sampled.toarray()))\n",
    "            # Update C_Sampled using optimization step\n",
    "            C_Sampled = csr_matrix(C_Sampled.multiply(T3_sampled.dot(V)) / (C_Sampled.dot(V.T).dot(V) + hy2 * C_Sampled))\n",
    "            # Update rows of C using C_Sampling indices\n",
    "            C[C_Sampling, :] = C_Sampled\n",
    "            # Estimate matrix EstimatedT4 using calculated matrices\n",
    "            EstimatedT4 = C.todense().dot(linalg.khatri_rao(A.todense(), B.todense()).T)\n",
    "            # Update delta_sampled with mu term\n",
    "            delta_sampled = delta_sampled + mu * (delta_sampled.T - delta_sampled)\n",
    "            # Update submatrix delta using delta_sampled and w_list indices\n",
    "            delta[w_list, :][:, w_list] = delta_sampled\n",
    "            if indomie%prin == 0:\n",
    "                try:\n",
    "                    factorization1 =  min(factorization, factorization1)\n",
    "                except:\n",
    "                    pass\n",
    "                factorization = rmse(T3, EstimatedT4).round(2)\n",
    "                plot.append([indomie, factorization])\n",
    "                abcd = pd.DataFrame(plot, columns = ['iteration', 'RMSE'])\n",
    "            if indomie%prin == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(\".............................................\")\n",
    "                print('iteration:                       ',indomie)\n",
    "                print('rmse factorization :             ', factorization )\n",
    "                try:\n",
    "                    print('rmse factorization min:          ', min(factorization, factorization1))\n",
    "                except:\n",
    "                    pass\n",
    "                print('.............................................')\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interruption detected. Returning...\")\n",
    "        return(A, B, C, W, delta)\n",
    "    return(A, B, C, W, delta)\n",
    "\n",
    "def ADGNTD_stochastic(Tens1, W_prior, r = 10, iter_steps = [1000, 2000, 3000],\n",
    "                      batchsize = [[20, 100, 20], [40,100,40], [60,100,60]], \n",
    "                      hy1=1, hy2=1, mu=0.1, factorization1=1000, max_iter = 6000,\n",
    "                      prin=50, A = None, B = None, C = None, W = None, delta = None ):\n",
    "    T3 = csr_matrix(unfolding_3D(Tens1, unfol_dim = 2, other_dim_seq = [0,1] ))\n",
    "    plot = []\n",
    "    try:\n",
    "        if (A == None) | (B == None)|  (C == None):\n",
    "            A = csr_matrix(np.random.rand(Tens1.shape[0],r))\n",
    "            B = csr_matrix(np.random.rand(Tens1.shape[1],r))\n",
    "            C = csr_matrix(np.random.rand(Tens1.shape[2],r))\n",
    "        if (W == None):\n",
    "            W = W_prior.copy()\n",
    "            delta = W*.5\n",
    "        if (delta == None):\n",
    "            delta = W*.5\n",
    "        A_list = list(range(A.shape[0]))\n",
    "        B_list = list(range(B.shape[0]))\n",
    "        C_list = list(range(C.shape[0]))\n",
    "        neum_term_B, denom_term_B = calculate_smoothed_weights(B)\n",
    "        start = 0\n",
    "        for indomie in range(max_iter):\n",
    "            try:\n",
    "                change_iter = iter_steps[start]\n",
    "            except:\n",
    "                change_iter = 100000000\n",
    "            if indomie >= change_iter:\n",
    "                start = start+1\n",
    "                #print('yes')\n",
    "            if indomie <= change_iter:\n",
    "                try:\n",
    "                    batchsize_selected = batchsize [start]\n",
    "                except:\n",
    "                    pass\n",
    "            #print(batchsize_selected)\n",
    "            A_Batch = int(batchsize_selected[0] / 100 * Tens1.shape[0] )\n",
    "            B_Batch = Tens1.shape[1]\n",
    "            C_Batch = int(batchsize_selected[2] / 100 * Tens1.shape[2] )\n",
    "            A_int = random.randrange(A.shape[0])\n",
    "            B_int = random.randrange(B.shape[0])\n",
    "            C_int = random.randrange(C.shape[0])\n",
    "            A_Sampling = random.sample(A_list, A_Batch)\n",
    "            B_Sampling = random.sample(B_list, B_Batch)\n",
    "            C_Sampling = random.sample(C_list, C_Batch)\n",
    "            A_Sampled = csr_matrix(A[A_Sampling, :])\n",
    "            B_Sampled = csr_matrix(B[B_Sampling, :])\n",
    "            C_Sampled = csr_matrix(C[C_Sampling, :])\n",
    "            Tens_sample = Tens1[A_Sampling, :, :][:, B_Sampling, :][:, :, C_Sampling]\n",
    "            T1_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=0, other_dim_seq=[1, 2]))\n",
    "            T2_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=1, other_dim_seq=[2, 0]))\n",
    "            T3_sampled = csr_matrix(unfolding_3D(Tens_sample, unfol_dim=2, other_dim_seq=[0, 1]))\n",
    "            # Loop through B_Sampling indices and create w_list_B\n",
    "            for index, j in enumerate(B_Sampling):\n",
    "                if index == 0:\n",
    "                    w_list_B = list(range(j * A.shape[0], (j + 1) * A.shape[0]))\n",
    "                if index > 0:\n",
    "                    w_list_B += list(range(j * A.shape[0], (j + 1) * A.shape[0]))\n",
    "            \n",
    "            # Loop through B indices and create w_list_A\n",
    "            for i in range(0, B.shape[0]):\n",
    "                if i == 0:\n",
    "                    w_list_A = list(np.array(A_Sampling) + i * A.shape[0])\n",
    "                if i > 0:\n",
    "                    w_list_A += list(np.array(A_Sampling) + i * A.shape[0])\n",
    "            \n",
    "            # Find common indices between w_list_A and w_list_B\n",
    "            w_list = list(set(w_list_A) & set(w_list_B))\n",
    "            # Sample submatrix W_sampled from W using w_list indices\n",
    "            W_sampled = W[w_list, :][:, w_list]\n",
    "            # Calculate degree matrix for sampled nodes\n",
    "            Deg_sampled = sparse.diags(np.squeeze(np.asarray(W_sampled.sum(axis=0)))).tocsr()\n",
    "            # Sample submatrix W_prior_sampled from W_prior using w_list indices\n",
    "            W_prior_sampled = W_prior[w_list, :][:, w_list]\n",
    "            # Sample submatrix delta_sampled from delta using w_list indices\n",
    "            delta_sampled = delta[w_list, :][:, w_list]\n",
    "            # Create matrix V using Khatri-Rao product of B_Sampled and C_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(B_Sampled.toarray(), C_Sampled.toarray()))\n",
    "            # Update A_Sampled using optimization step\n",
    "            A_Sampled = csr_matrix(A_Sampled.multiply(T1_sampled.dot(V) \n",
    "                                                      + hy1 * derivative_dynamic_graph_A_W(A_Sampled, B_Sampled, W_sampled)) / \n",
    "                                   (A_Sampled.dot(V.T).dot(V) + hy2 * A_Sampled + \n",
    "                                    hy1 * derivative_dynamic_graph_A_D(A_Sampled, B_Sampled, Deg_sampled)))\n",
    "            # Update rows of A using A_Sampling indices\n",
    "            A[A_Sampling, :] = A_Sampled\n",
    "            # Recalculate matrix V using Khatri-Rao product of C_Sampled and A_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(C_Sampled.toarray(), A_Sampled.toarray()))\n",
    "            # Update B_Sampled using optimization step\n",
    "            B_num = B_Sampled.multiply(csr_matrix(T2_sampled.dot(V)) + 5 * (neum_term_B.dot(B)) \n",
    "                                       + hy1 * derivative_dynamic_graph_B_W(A_Sampled, B_Sampled, W_sampled))\n",
    "            B_denom = (B_Sampled.dot(V.T).dot(V) + hy2 * B_Sampled + 5 * (denom_term_B.dot(B)) \n",
    "                       + hy1 * derivative_dynamic_graph_B_D(A_Sampled, B_Sampled, Deg_sampled))\n",
    "            B_Sampled = csr_matrix(B_num / B_denom)\n",
    "            # Update rows of B using B_Sampling indices\n",
    "            B[B_Sampling, :] = B_Sampled\n",
    "            # Recalculate matrix V using Khatri-Rao product of A_Sampled and B_Sampled\n",
    "            V = csr_matrix(linalg.khatri_rao(A_Sampled.toarray(), B_Sampled.toarray()))\n",
    "            # Update C_Sampled using optimization step\n",
    "            C_Sampled = csr_matrix(C_Sampled.multiply(T3_sampled.dot(V)) / (C_Sampled.dot(V.T).dot(V) + hy2 * C_Sampled))\n",
    "            # Update rows of C using C_Sampling indices\n",
    "            C[C_Sampling, :] = C_Sampled\n",
    "            # Estimate matrix EstimatedT4 using calculated matrices\n",
    "            EstimatedT4 = C.todense().dot(linalg.khatri_rao(A.todense(), B.todense()).T)\n",
    "            # Update delta_sampled with mu term\n",
    "            delta_sampled = delta_sampled + mu * (delta_sampled.T - delta_sampled)\n",
    "            # Update submatrix delta using delta_sampled and w_list indices\n",
    "            delta[w_list, :][:, w_list] = delta_sampled\n",
    "            if indomie%prin == 0:\n",
    "                try:\n",
    "                    factorization1 =  min(factorization, factorization1)\n",
    "                except:\n",
    "                    pass\n",
    "                factorization = rmse(T3, EstimatedT4).round(2)\n",
    "                plot.append([indomie, factorization])\n",
    "                abcd = pd.DataFrame(plot, columns = ['iteration', 'RMSE'])\n",
    "            if indomie%prin == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(\".............................................\")\n",
    "                print('iteration:                       ',indomie)\n",
    "                print('rmse factorization :             ', factorization )\n",
    "                try:\n",
    "                    print('rmse factorization min:          ', min(factorization, factorization1))\n",
    "                    print('batchsize_selected:          ', batchsize_selected)\n",
    "                    #batchsize_selected\n",
    "                except:\n",
    "                    pass\n",
    "                print('.............................................')\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interruption detected. Returning...\")\n",
    "        return(A, B, C, W, delta)\n",
    "        \n",
    "    return(A, B, C, W, delta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed299570-b3c3-4fd0-a499-00bc7a7c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pattern mining code\n",
    "def Data_Structuring(X):\n",
    "    W = np.array((X - X + 1).replace(np.nan, 0))\n",
    "    RW = np.array((X - X).replace(np.nan, 1))\n",
    "    Y = np.array(X.replace(np.nan, 0))\n",
    "    beta = np.sqrt((Y* Y).sum().sum())\n",
    "    dict_temp = {}\n",
    "    parms = [W, RW, Y/beta, beta, Y]\n",
    "    for i in [0,1,2,3, 4]:\n",
    "        dict_temp[i] = parms[i]\n",
    "    return (dict_temp)\n",
    "def get_ssim(vec_1, vec_2, **kwargs):\n",
    "    if type(vec_1) != type(pd.Series(dtype = int)) or type(vec_2) != type(pd.Series(dtype = int)):\n",
    "        raise TypeError(\"Input vectors should be pandas series dataypes\")\n",
    "    try:\n",
    "        alpha = kwargs['alpha']\n",
    "    except:\n",
    "        alpha = 1\n",
    "        #print(\"Alpha value not provided, using default value (1)\")\n",
    "    try:\n",
    "        beta = kwargs['beta']\n",
    "    except:\n",
    "        beta = 1\n",
    "        #print(\"Beta value not provided, using default value (1)\")\n",
    "    try:\n",
    "        gamma = kwargs['gamma']\n",
    "    except:\n",
    "        gamma = 1\n",
    "        #print(\"Gamma value not provided, using default value (1)\")\n",
    "    c_1 = 0.0\n",
    "    c_2 = 0.0\n",
    "    c_3 = 0.0\n",
    "    l = (2 * (vec_1.mean() * vec_2.mean()) + c_1) / (vec_1.mean() ** 2 + vec_2.mean()**2 + c_1)\n",
    "    c = (2 * np.sqrt(vec_1.var()) * np.sqrt(vec_2.var()) + c_2) / (vec_1.var() + vec_2.var() + c_2)\n",
    "    s = (((vec_1 - vec_1.mean()) * (vec_2 - vec_2.mean())).sum()/(vec_1.shape[0]-1) + c_3) / (np.sqrt(vec_1.var()) * np.sqrt(vec_2.var()) + c_3 )\n",
    "    #print(l, c, s)\n",
    "    #print(((2 * vec_1.mean() * vec_2.mean())*(2 *  vec_1.cov(vec_2))) / ((vec_1.mean() **2 + vec_2.mean() ** 2) * (vec_1.var() ** 2 + vec_2.var()) ))\n",
    "    return (l ** alpha) * (c ** beta) * (s ** gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7742c7d-04eb-4c6d-b62a-dce71b491703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline encoding\n",
    "def r_squared(ytrue,ypred):\n",
    "    if len(ytrue) == len(ypred):\n",
    "        pred_mean = ypred.mean()\n",
    "        var_mean = np.power((ypred-pred_mean),2)\n",
    "        var_line = np.power((ytrue-ypred),2)\n",
    "        r_sq = 1 - ((var_line.sum())/(var_mean.sum()))\n",
    "        return r_sq\n",
    "    else:\n",
    "        print('Lengths Dont Match')\n",
    "def initialize_weights_and_V(ind_1, day, Tens_A, data_shubham, Patterns, Patterns_B, \n",
    "                             current_day_original, time_of_day, weight_future, weight_current_pred):\n",
    "    # Convert date string to datetime object\n",
    "    date = datetime.datetime.strptime(ind_1.index[(day-0) * Tens_A.shape[2]: (day-0+1) * Tens_A.shape[2]][0][0], '%Y-%m-%d')\n",
    "    day_of_week_current = date.strftime('%A')\n",
    "    #print(data_shubham)\n",
    "    # Extract test data for the current day\n",
    "    test_data_current_day = data_shubham[data_shubham.index.isin(ind_1.index[(day-0) * Tens_A.shape[2]: (day-0+1) * Tens_A.shape[2]])]\n",
    "    #print('a', test_data_current_day)\n",
    "    current_day_comparison = test_data_current_day.reset_index(drop=True).iloc[0:time_of_day, :]\n",
    "    current_day_comparison = current_day_comparison.reset_index(drop=True)\n",
    "    #print('b', current_day_comparison)\n",
    "    #print(Tens_A.shape)\n",
    "    Selection_list = []\n",
    "    # Loop through unique patterns\n",
    "    for pattern in (Patterns['day_of_week']).unique():\n",
    "        #print(pattern)\n",
    "        Patterns_temp = Patterns.copy(deep=True)\n",
    "        Patterns_temp = Patterns_temp[Patterns_temp['day_of_week'] == pattern]\n",
    "        \n",
    "        del Patterns_temp['day_of_week']\n",
    "        del Patterns_temp['time']\n",
    "        #print(Patterns_temp.shape)\n",
    "        Patterns_temp = Patterns_temp.iloc[0:time_of_day, :].reset_index(drop=True)\n",
    "        #print(Patterns_temp.shape, current_day_comparison.shape)\n",
    "        # Calculate r_squared and average\n",
    "        Selection_list.append([pattern, np.array(r_squared(Patterns_temp.unstack(0), current_day_comparison.unstack(0))).mean()])\n",
    "    \n",
    "    # Create a DataFrame to select the best pattern\n",
    "    Selection_df = pd.DataFrame(Selection_list).sort_values(by=1, ascending=False)\n",
    "    selected_pattern = Selection_df[[0]].iloc[0][0]\n",
    "    train_data_shubham_temp_plot = Patterns[Patterns['day_of_week'] == selected_pattern].reset_index(drop=True)\n",
    "    \n",
    "    #B_half = np.array(Patterns_B[Patterns_B['cluster_id'] == selected_pattern].reset_index(drop=True).T.iloc[1:].astype('float64')).T\n",
    "    #print(B_half)\n",
    "    B_half = np.array(Patterns_B.mean(axis = 0).reset_index(drop=True).T.iloc[1:].astype('float64')).T\n",
    "    B_half = B_half.reshape(1, B_half.shape[0])\n",
    "    # Calculate current_day_original_Weights\n",
    "    current_day_original_Weights = current_day_original.copy()\n",
    "    current_day_original_Weights = (current_day_original_Weights + 0.00000001) / (current_day_original_Weights + 0.00000001) + weight_current_pred\n",
    "    current_day_original_Weights[:, time_of_day:] = 0\n",
    "    current_day_original_Weights_Opposite = current_day_original_Weights.copy()\n",
    "    current_day_original_Weights_Opposite[current_day_original_Weights_Opposite == 0] = 2\n",
    "    current_day_original_Weights_Opposite[current_day_original_Weights_Opposite == 1] = 0\n",
    "    current_day_original_Weights_Opposite[current_day_original_Weights_Opposite == 2] = 1\n",
    "    \n",
    "    # Initialize V_initialize\n",
    "    hist_val = cp.asarray(np.array(train_data_shubham_temp_plot[train_data_shubham_temp_plot.columns[2:]]).T)\n",
    "    current_day_original[:, time_of_day + 8:] = hist_val[:, time_of_day + 8:]\n",
    "    current_day_original_Weights[:, time_of_day + 8:] = weight_future\n",
    "    #print(B_half.shape, C.shape)\n",
    "    V_initialize_start = cp.asarray(linalg.khatri_rao(B_half, C))\n",
    "    V_initialize = V_initialize_start.copy()\n",
    "    current_day_original_Weights = cp.asarray(current_day_original_Weights)\n",
    "    current_day_original = cp.asarray(current_day_original)\n",
    "    V_initialize = cp.asarray(V_initialize)\n",
    "    \n",
    "    return current_day_original_Weights, V_initialize, hist_val, current_day_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fff7ecf-de5a-483a-a986-c39b15e16781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_encoding (Tens_A, data_shubham, A, B, C, ind,rolling_window, future_time_steps, hyper_smooth, \n",
    "                      hyp_previous_reg , weight_futuree, weight_current_pred,last_time = 88,\n",
    "                      yTrain1final = None, xTrain1final = None, day_max = 20):\n",
    "    print(hyp_previous_reg)\n",
    "    try:\n",
    "        #print('lt', last_time)\n",
    "        try:\n",
    "            if yTrain1final == None:\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag = 1\n",
    "            day_start = 0\n",
    "        except:\n",
    "            flag = 1\n",
    "            day_start = int(yTrain1final.shape[0] / (last_time - rolling_window))\n",
    "            pass\n",
    "        #print('flag...........................................................')\n",
    "        # Calculate magnitude of matrix A and its inverse\n",
    "        mag_A = np.diag(np.asarray(np.sqrt((A.T.dot(A)).sum(axis=1))).reshape(-1))\n",
    "        mag_A_inverse = np.linalg.inv(mag_A)\n",
    "        \n",
    "        # Create a DataFrame 'data' using dot product and transpose\n",
    "        data = pd.DataFrame((mag_A).dot(linalg.khatri_rao(B, C).T).T).round(5)\n",
    "        \n",
    "        # Find the maximum value in 'data'\n",
    "        data_max = np.array(data).max()\n",
    "        \n",
    "        # Copy arrays to GPU if available\n",
    "        A_dash = cp.asarray(A.copy())\n",
    "        Tens_A = cp.asarray(Tens_A)\n",
    "        mag_A = cp.array(mag_A)\n",
    "\n",
    "        \n",
    "        # Loop through days and time of day\n",
    "        for day in range(day_start, day_max):\n",
    "            a = time.time()\n",
    "            print(day)\n",
    "            current_day_OO = Tens_A[:, day, :].copy()\n",
    "            for time_of_day in range(rolling_window, last_time):\n",
    "                #print(day, time_of_day)\n",
    "                #print('lt', last_time)\n",
    "                # Extract current day data\n",
    "                current_day_original  = current_day_OO.copy()\n",
    "                #current_day_original[:, time_of_day:] = 20 + random.randint(0, 10)\n",
    "                #print(current_day_original)\n",
    "                # Initialize weights, V, and history value\n",
    "                current_day_original_Weights, V_initialize, hist_val, current_day_original = initialize_weights_and_V(ind, \n",
    "                                                                                                                      day, Tens_A,\n",
    "                                                                                                                      data_shubham, Patterns, Patterns_B, \n",
    "                                                                                                                      current_day_original, time_of_day, weight_future,weight_current_pred)\n",
    "                if time_of_day == rolling_window:\n",
    "                    V_initialize = V_initialize.copy()\n",
    "                if time_of_day!=rolling_window:\n",
    "                    V_dash.copy()\n",
    "                V_initialize = cp.asarray(V_initialize)\n",
    "                if (flag == 0) | (day_start!=0):\n",
    "                    # Calculate numerator and denominator terms for weight smoothing\n",
    "                    neum_term_C2, denom_term_C2 = calculate_smoothed_weights_cp(V_initialize)\n",
    "                neum_V_new = (current_day_original_Weights.T * current_day_original.T).dot(A_dash)\n",
    "                for i in range(0, 30):\n",
    "                    # Update V using weighted dot products\n",
    "                    \n",
    "                    denom_V_new = (current_day_original_Weights.T * V_initialize.dot(A_dash.T)).dot(A_dash)\n",
    "        \n",
    "                    # Update V_initialize using calculated terms\n",
    "                    V_initialize = V_initialize * (((neum_V_new + hyper_smooth * neum_term_C2.dot(V_initialize) + 0.000000001) /\n",
    "                                                    (denom_V_new + 10 * V_initialize + \n",
    "                                                     hyper_smooth * denom_term_C2.dot(V_initialize) \n",
    "                                                     + 0.000000001)))\n",
    "                #print(time_of_day)\n",
    "        \n",
    "                # Store previous values\n",
    "                V_initialize_previous = V_initialize.copy()\n",
    "                current_day_original_Weights_previous = current_day_original_Weights.copy()\n",
    "        \n",
    "                # Calculate V using magnitude of A and V_initialize\n",
    "                V = (mag_A).dot(V_initialize.T).T/data_max\n",
    "                \n",
    "                # Generate training sequences\n",
    "                xTrain1 = V[time_of_day - rolling_window: time_of_day, :]\n",
    "                xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "        \n",
    "                # Concatenate or copy xTrain1_subset based on flag value\n",
    "                if flag == 1:\n",
    "                    xTrain1final = np.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "                elif flag == 0:\n",
    "                    xTrain1final = xTrain1.copy()\n",
    "        \n",
    "                # Update weights for current day\n",
    "                Weight_temp = V_initialize.copy()\n",
    "                Weight_temp[time_of_day - rolling_window: time_of_day, :] = hyp_previous_reg\n",
    "                Weight_temp[0: time_of_day - rolling_window, :] = 0\n",
    "                Weight_temp[time_of_day:, :] = 0\n",
    "        \n",
    "                current_day_original  = current_day_OO.copy()\n",
    "        \n",
    "                # Update current day weights and values\n",
    "                current_day_original_Weights = np.divide(current_day_original + 0.00000001, current_day_original + 0.00000001)\n",
    "                current_day_original_Weights[:, time_of_day + future_time_steps:] = 1\n",
    "                #current_day_original_Weights_Opposite = 1 - current_day_original_Weights\n",
    "        \n",
    "                V_initialize = V_initialize.copy()\n",
    "                #current_day_original[:, time_of_day + 16:] = hist_val[:, time_of_day + 16:]\n",
    "                current_day_original_Weights[:, time_of_day:] = weight_current_pred\n",
    "                current_day_original_Weights[:, time_of_day:time_of_day + future_time_steps] = weight_current_pred\n",
    "                t1 = (current_day_original_Weights.T * current_day_original.T).dot(A_dash) \n",
    "                for i in range(0,300):\n",
    "                    # Update V using weighted dot products and smoothing terms\n",
    "                    neum_V_new = t1+ Weight_temp * V_initialize_previous\n",
    "                    denom_V_new = (current_day_original_Weights.T * \n",
    "                                   V_initialize.dot(A_dash.T)).dot(A_dash) +  Weight_temp * V_initialize\n",
    "                    V_initialize = V_initialize * (((neum_V_new + hyper_smooth * neum_term_C2.dot(V_initialize) + 0.000000001) /\n",
    "                                                    (denom_V_new + 10 * V_initialize +\n",
    "                                                     hyper_smooth * denom_term_C2.dot(V_initialize) + 0.000000001)))\n",
    "        \n",
    "                # Update V using magnitude of A and V_initialize\n",
    "                V = (mag_A).dot(V_initialize.T).T / data_max\n",
    "                #print('hello', V.shape, time_of_day, time_of_day, time_of_day + future_time_steps)\n",
    "                #time.sleep(5) \n",
    "                # Generate prediction sequences\n",
    "                xPred1 = V[time_of_day - rolling_window: time_of_day + future_time_steps, :]\n",
    "                xPred1 = xPred1.reshape(1, *xPred1.shape)\n",
    "        \n",
    "                # Concatenate or copy yTrain1final based on flag value\n",
    "                if flag == 1:\n",
    "                    #print(yTrain1final.shape, xPred1.shape)\n",
    "                    yTrain1final = np.concatenate((yTrain1final, xPred1), axis=0)\n",
    "                elif flag == 0:\n",
    "                    yTrain1final = xPred1.copy()\n",
    "                    flag = 1\n",
    "                V_dash =  V_initialize_previous.copy()\n",
    "                V_dash[:time_of_day, :] = V_initialize[:time_of_day,: ].copy()\n",
    "                # Calculate error and print results\n",
    "            #print(current_day_original_Weights[:, time_of_day:time_of_day + future_time_steps])\n",
    "            err = current_day_original[:, time_of_day: time_of_day + future_time_steps] - A_dash.dot(V_initialize.T)[:, time_of_day: time_of_day + future_time_steps]\n",
    "            #print(err)\n",
    "            clear_output(wait=True)\n",
    "            #print(current_day_original[:, time_of_day: time_of_day + future_time_steps], A_dash.dot(V_initialize.T)[:, time_of_day: time_of_day + future_time_steps])\n",
    "            print(i, day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])))\n",
    "    \n",
    "            # Print timing and shapes\n",
    "            #print(time.time() - a)\n",
    "            #print(yTrain1final.shape, xTrain1final.shape, day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interruption detected. Returning...\")\n",
    "        valid_entries = math.floor(yTrain1final.shape[0] / (last_time - rolling_window)) * (last_time - rolling_window)\n",
    "        print('valid entrieS ',valid_entries )\n",
    "        return yTrain1final[0: valid_entries, :, :], xTrain1final[0: valid_entries, :, :]\n",
    "    return yTrain1final, xTrain1final\n",
    "\n",
    "def Generate_original_sequence_arrays(Tens_A, rolling_window = 20, future_time_steps = 4, max_days = 270, last_time = 88):\n",
    "    Index_final = []\n",
    "    for day in range(0, max_days):\n",
    "        #print(day)\n",
    "        for time_of_day in range(rolling_window, 88):\n",
    "            Index_final.append([day, time_of_day])\n",
    "            \n",
    "    flag = 11\n",
    "    xTrain1true_Original = None\n",
    "    yTrain1true_Original = None\n",
    "    \n",
    "    for day in range(0, max_days):\n",
    "        #print(day)\n",
    "        for time_of_day in range(20, last_time):\n",
    "            day_data = Tens_A[:, day, :]  # You need to provide Tens_A\n",
    "            \n",
    "            #xTrain1_subset_Original = cp.asarray(day_data[:, time_of_day - rolling_window: time_of_day].T)\n",
    "            #xTrain1_subset_Original = xTrain1_subset_Original.reshape(1, *xTrain1_subset_Original.shape)\n",
    "            \n",
    "            yTrain1_subset_Original = cp.asarray(day_data[:, time_of_day: time_of_day + future_time_steps].T)\n",
    "            yTrain1_subset_Original = yTrain1_subset_Original.reshape(1, *yTrain1_subset_Original.shape)\n",
    "            \n",
    "            if flag == 10:\n",
    "                #xTrain1true_Original = cp.concatenate((xTrain1true_Original, xTrain1_subset_Original), axis=0)\n",
    "                yTrain1true_Original = cp.concatenate((yTrain1true_Original, yTrain1_subset_Original), axis=0)\n",
    "            if flag == 11:\n",
    "                #xTrain1true_Original = xTrain1_subset_Original.copy()\n",
    "                yTrain1true_Original = yTrain1_subset_Original.copy()\n",
    "                #print('Ok...............')\n",
    "                flag = 10\n",
    "        clear_output(wait=True)\n",
    "        print(day)\n",
    "        #print(day, yTrain1true_Original.shape, xTrain1true_Original.shape)\n",
    "    return Index_final, yTrain1true_Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02af2a4-71ed-4d80-9850-d7dced12e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Offline_encoding_graph_stochastic(Tens_A, data_shubham, A, B, C, Deg, W, ind_1,rolling_window, future_time_steps, hyper_smooth, \n",
    "                      hyp_previous_reg , weight_future, weight_current_pred,last_time = 88,\n",
    "                      yTrain1final = None, xTrain1final = None, day_max = 20, hy1 = 0.01):\n",
    "    try:\n",
    "        C_list = list(range(C.shape[0]))\n",
    "        #print('lt', last_time)\n",
    "        try:\n",
    "            if yTrain1final == None:\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag = 1\n",
    "            day_start = 0\n",
    "        except:\n",
    "            flag = 1\n",
    "            day_start = int(yTrain1final.shape[0] / (last_time - rolling_window))\n",
    "            pass\n",
    "        #print('flag...........................................................')\n",
    "        # Calculate magnitude of matrix A and its inverse\n",
    "        mag_A = np.diag(np.asarray(np.sqrt((A.T.dot(A)).sum(axis=1))).reshape(-1))\n",
    "        mag_A_inverse = np.linalg.inv(mag_A)\n",
    "        \n",
    "        # Create a DataFrame 'data' using dot product and transpose\n",
    "        data = pd.DataFrame((mag_A).dot(linalg.khatri_rao(B, C).T).T).round(5)\n",
    "        \n",
    "        # Find the maximum value in 'data'\n",
    "        data_max = np.array(data).max()\n",
    "        \n",
    "        # Copy arrays to GPU if available\n",
    "        A_dash = cp.asarray(A.copy())\n",
    "        Tens_A = cp.asarray(Tens_A)\n",
    "        mag_A = cp.array(mag_A)\n",
    "        complete_list = list(range(0, C.shape[0]))\n",
    "\n",
    "        W = coo_matrix(cp.asarray(W.toarray()))\n",
    "        In_n = coo_matrix(cp.identity(A_dash.shape[1]))\n",
    "        I_dash_n = coo_matrix(diags(cp.ones((W.shape[0],))))\n",
    "        temp_neum_term_n = coo_matrix(cp.asarray(linalg.khatri_rao(In_n.get().toarray(), A_dash.get())))\n",
    "        kron1_n = kron(In_n.T , W)\n",
    "        Deg = coo_matrix(cp.asarray(Deg.toarray()))\n",
    "        In_d = csr_matrix(cp.identity(A_dash.shape[1]))\n",
    "        khatri_rao_term_d = coo_matrix(cp.asarray(linalg.khatri_rao(In_d.get().toarray(), A_dash.get())))\n",
    "        kron_term_d = kron(In_d.T , Deg)\n",
    "        abcd = time.time()\n",
    "        for day in range(day_start, day_max):\n",
    "            a = time.time()\n",
    "            #print(day)\n",
    "            current_day_OO = Tens_A[:, day, :].copy()\n",
    "            for time_of_day in range(rolling_window, last_time):\n",
    "                a = time.time()\n",
    "                #print('lt', last_time)\n",
    "                # Extract current day data\n",
    "                current_day_original  = current_day_OO.copy()\n",
    "                #current_day_original[:, time_of_day:] = 20 + random.randint(0, 10)\n",
    "                #print(current_day_original)\n",
    "                # Initialize weights, V, and history value\n",
    "                #print(ind_1)\n",
    "                current_day_original_Weights, V_initialize, hist_val, current_day_original = initialize_weights_and_V(ind_1, \n",
    "                                                                                                                      day, Tens_A,\n",
    "                                                                                                                      data_shubham, Patterns, Patterns_B, current_day_original, time_of_day, weight_future)\n",
    "                \n",
    "                V_initialize = cp.asarray(V_initialize)\n",
    "                if (flag == 0) | (day_start!=0):\n",
    "                    # Calculate numerator and denominator terms for weight smoothing\n",
    "                    neum_term_C2, denom_term_C2 = calculate_smoothed_weights_cp(V_initialize)\n",
    "                for i in range(0, 2):\n",
    "                    # Update V using weighted dot products\n",
    "                    neum_V_new = (current_day_original_Weights.T * current_day_original.T).dot(A_dash)\n",
    "                    denom_V_new = (current_day_original_Weights.T * V_initialize.dot(A_dash.T)).dot(A_dash)\n",
    "        \n",
    "                    # Update V_initialize using calculated terms\n",
    "                    V_initialize = V_initialize * (((neum_V_new + hyper_smooth * neum_term_C2.dot(V_initialize) + 0.000000001) /\n",
    "                                                    (denom_V_new + 10 * V_initialize + \n",
    "                                                     hyper_smooth * denom_term_C2.dot(V_initialize) \n",
    "                                                     + 0.000000001)))\n",
    "                neum_V_new = (current_day_original_Weights.T * current_day_original.T).dot(A_dash)\n",
    "                for i in range(0, 30):\n",
    "                    current_list = list(range(time_of_day -rolling_window, time_of_day))\n",
    "                    random_numbers = random.sample(complete_list, 15)\n",
    "                    combined_set = sorted(set(current_list + random_numbers))  # Combine lists and convert to set\n",
    "                    range_list = [list(range(A.shape[0] * num, A.shape[0] * (num + 1))) for num in combined_set]\n",
    "                    new_list = list(chain.from_iterable(range_list))\n",
    "                    W_sample = coo_matrix(W.toarray()[new_list, : ][:, new_list])\n",
    "                    Deg_sample = diags(cp.squeeze(cp.asarray(W_sample.sum(axis = 0)))).tocsr()\n",
    "                    \n",
    "                    denom_V_new = (current_day_original_Weights[:, combined_set].T * V_initialize[combined_set, :].dot(A_dash.T)).dot(A_dash)\n",
    "                    I_dash_n_sample = coo_matrix(diags(cp.ones((W_sample.shape[0],))))\n",
    "                    kron1_n_sample = kron(In_n.T , W_sample)\n",
    "                    neum_graph = cp.asarray(hy1 *  derivative_dynamic_graph_B_W_cp(coo_matrix(A_dash),\n",
    "                                                                                        coo_matrix(V_initialize[combined_set, :]),W_sample,In_n, I_dash_n_sample,temp_neum_term_n,kron1_n_sample ).toarray())\n",
    "                    kron_term_d_sample = kron(In_d.T , Deg_sample)\n",
    "                    denom_graph = cp.asarray(hy1 * derivative_dynamic_graph_B_D_cp(coo_matrix(A_dash),\n",
    "                                                                                   coo_matrix(V_initialize[combined_set, :]), Deg_sample,In_d,khatri_rao_term_d, kron_term_d_sample ).toarray())\n",
    "                    V_initialize[combined_set, :] = V_initialize[combined_set, :] * (((neum_V_new[combined_set, :] + hyper_smooth * neum_term_C2[combined_set, :][:, combined_set].dot(V_initialize[combined_set, :]) + 0.000000001 + neum_graph) /\n",
    "                                                        (denom_V_new + 100 * V_initialize[combined_set, :] + \n",
    "                                                         hyper_smooth * denom_term_C2[combined_set, :][:, combined_set].dot(V_initialize[combined_set, :]) \n",
    "                                                         + 0.000000001 + denom_graph)))\n",
    "                #err = current_day_original[:, time_of_day-4: time_of_day ] - A_dash.dot(V_initialize.T)[:, time_of_day-4: time_of_day]\n",
    "                #print(day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])), time.time() - a)\n",
    "                V_initialize_previous = V_initialize.copy()\n",
    "                current_day_original_Weights_previous = current_day_original_Weights.copy()\n",
    "        \n",
    "                # Calculate V using magnitude of A and V_initialize\n",
    "                V = (mag_A).dot(V_initialize.T).T/data_max\n",
    "                \n",
    "                # Generate training sequences\n",
    "                xTrain1 = V[time_of_day - rolling_window: time_of_day, :]\n",
    "                xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "        \n",
    "                # Concatenate or copy xTrain1_subset based on flag value\n",
    "                if flag == 1:\n",
    "                    xTrain1final = np.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "                elif flag == 0:\n",
    "                    xTrain1final = xTrain1.copy()\n",
    "        \n",
    "                # Update weights for current day\n",
    "                Weight_temp = V_initialize.copy()\n",
    "                Weight_temp[0: time_of_day - rolling_window] = 0\n",
    "                Weight_temp[time_of_day:] = 0\n",
    "        \n",
    "                current_day_original  = current_day_OO.copy()\n",
    "        \n",
    "                # Update current day weights and values\n",
    "                current_day_original_Weights = np.divide(current_day_original + 0.00000001, current_day_original + 0.00000001)\n",
    "                current_day_original_Weights[:, time_of_day + future_time_steps:] = 1\n",
    "                #current_day_original_Weights_Opposite = 1 - current_day_original_Weights\n",
    "        \n",
    "                V_initialize = V_initialize_previous.copy()\n",
    "                current_day_original[:, time_of_day + 16:] = hist_val[:, time_of_day + 16:]\n",
    "                current_day_original_Weights[:, time_of_day + 16:] = weight_future\n",
    "                current_day_original_Weights[:, time_of_day:time_of_day + future_time_steps] = weight_current_pred\n",
    "                for i in range(0,2):\n",
    "                    # Update V using weighted dot products and smoothing terms\n",
    "                    neum_V_new = (current_day_original_Weights.T * current_day_original.T).dot(A_dash) + hyp_previous_reg * Weight_temp * V_initialize_previous\n",
    "                    denom_V_new = (current_day_original_Weights.T * V_initialize.dot(A_dash.T)).dot(A_dash) + hyp_previous_reg * Weight_temp * V_initialize\n",
    "                    V_initialize = V_initialize * (((neum_V_new + hyper_smooth * neum_term_C2.dot(V_initialize) + 0.000000001) /\n",
    "                                                    (denom_V_new + 10 * V_initialize + hyper_smooth * denom_term_C2.dot(V_initialize) + 0.000000001)))\n",
    "        \n",
    "                neum_V_new = (current_day_original_Weights.T * current_day_original.T).dot(A_dash) + hyp_previous_reg * Weight_temp * V_initialize_previous\n",
    "                \n",
    "                for i in range(0, 30):\n",
    "                    current_list = list(range(time_of_day -rolling_window, time_of_day + future_time_steps))\n",
    "                    random_numbers = random.sample(complete_list, 15)\n",
    "                    combined_set = sorted(set(current_list + random_numbers))  # Combine lists and convert to set\n",
    "                    range_list = [list(range(A.shape[0] * num, A.shape[0] * (num + 1))) for num in combined_set]\n",
    "                    new_list = list(chain.from_iterable(range_list))\n",
    "                    W_sample = coo_matrix(W.toarray()[new_list, : ][:, new_list])\n",
    "                    Deg_sample = diags(cp.squeeze(cp.asarray(W_sample.sum(axis = 0)))).tocsr()\n",
    "                    \n",
    "                    denom_V_new = (current_day_original_Weights[:, combined_set].T * V_initialize[combined_set, :].dot(A_dash.T)).dot(A_dash) +  hyp_previous_reg * Weight_temp[combined_set, :] * V_initialize[combined_set, :]\n",
    "                    #denom_V_new = (current_day_original_Weights.T * V_initialize.dot(A_dash.T)).dot(A_dash) + hyp_previous_reg * Weight_temp * V_initialize\n",
    "                    I_dash_n_sample = coo_matrix(diags(cp.ones((W_sample.shape[0],))))\n",
    "                    kron1_n_sample = kron(In_n.T , W_sample)\n",
    "                    neum_graph = cp.asarray(hy1 *  derivative_dynamic_graph_B_W_cp(coo_matrix(A_dash),\n",
    "                                                                                        coo_matrix(V_initialize[combined_set, :]),W_sample,In_n, I_dash_n_sample,temp_neum_term_n,kron1_n_sample ).toarray())\n",
    "                    kron_term_d_sample = kron(In_d.T , Deg_sample)\n",
    "                    denom_graph = cp.asarray(hy1 * derivative_dynamic_graph_B_D_cp(coo_matrix(A_dash),\n",
    "                                                                                   coo_matrix(V_initialize[combined_set, :]), Deg_sample,In_d,khatri_rao_term_d, kron_term_d_sample ).toarray())\n",
    "                    V_initialize[combined_set, :] = V_initialize[combined_set, :] * (((neum_V_new[combined_set, :] + hyper_smooth * neum_term_C2[combined_set, :][:, combined_set].dot(V_initialize[combined_set, :]) + 0.000000001 + neum_graph) /\n",
    "                                                        (denom_V_new + 100 * V_initialize[combined_set, :] + \n",
    "                                                         hyper_smooth * denom_term_C2[combined_set, :][:, combined_set].dot(V_initialize[combined_set, :]) \n",
    "                                                         + 0.000000001 + denom_graph)))\n",
    "        \n",
    "                #err = current_day_original[:, time_of_day: time_of_day +4 ] - A_dash.dot(V_initialize.T)[:, time_of_day: time_of_day +4]\n",
    "                #print(day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])), time.time() - a)\n",
    "                V = (mag_A).dot(V_initialize.T).T / data_max\n",
    "                #print('hello', V.shape, time_of_day, time_of_day, time_of_day + future_time_steps)\n",
    "                #time.sleep(5) \n",
    "                # Generate prediction sequences\n",
    "                xPred1 = V[time_of_day - rolling_window: time_of_day + future_time_steps, :]\n",
    "                xPred1 = xPred1.reshape(1, *xPred1.shape)\n",
    "        \n",
    "                # Concatenate or copy yTrain1final based on flag value\n",
    "                if flag == 1:\n",
    "                    #print(yTrain1final.shape, xPred1.shape)\n",
    "                    yTrain1final = np.concatenate((yTrain1final, xPred1), axis=0)\n",
    "                elif flag == 0:\n",
    "                    yTrain1final = xPred1.copy()\n",
    "                    flag = 1\n",
    "        \n",
    "                # Calculate error and print results\n",
    "                #print(current_day_original_Weights[:, time_of_day:time_of_day + future_time_steps])\n",
    "                if time_of_day%1 == 0:\n",
    "                    err = current_day_original[:, time_of_day: time_of_day + future_time_steps] - A_dash.dot(V_initialize.T)[:, time_of_day: time_of_day + future_time_steps]\n",
    "                    #print(err)\n",
    "                    clear_output(wait=True)\n",
    "                    #print(current_day_original[:, time_of_day: time_of_day + future_time_steps], A_dash.dot(V_initialize.T)[:, time_of_day: time_of_day + future_time_steps])\n",
    "                    print(day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])), time.time() - abcd)\n",
    "    \n",
    "        # Print timing and shapes\n",
    "        #print(time.time() - a)\n",
    "        #print(yTrain1final.shape, xTrain1final.shape, day, time_of_day, np.sqrt((err * err).sum() / (err.shape[0] * err.shape[1])))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Keyboard interruption detected. Returning...\")\n",
    "        valid_entries = math.floor(yTrain1final.shape[0] / (last_time - rolling_window)) * (last_time - rolling_window)\n",
    "        print('valid entrieS ',valid_entries )\n",
    "        return yTrain1final[0: valid_entries, :, :], xTrain1final[0: valid_entries, :, :]\n",
    "    return yTrain1final, xTrain1final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe7154-dff9-421d-b7ad-ee41eed29050",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "hy1 = 0.001\n",
    "flag = 0\n",
    "hy_prev = 100\n",
    "from cupyx.scipy.sparse import csr_matrix, diags\n",
    "for day in range(0, 1):\n",
    "    a = time.time()\n",
    "    current_day_OO = Tens[:, day, :].copy()\n",
    "    e = 0.00000001\n",
    "    A_cp = cp.asarray(A.copy())\n",
    "    for time_of_day in range(rolling_window, last_time):\n",
    "        for iteration in range(0, 25):\n",
    "            if iteration == 0:\n",
    "                rolling_window_array = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day])\n",
    "                V_int = cp.random.rand(rolling_window_array.shape[1],r)\n",
    "                W_temp = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "                W_temp = W_temp[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "                Deg_temp = Deg[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "                Deg_temp = Deg_temp[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "                kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "            if iteration >=0:\n",
    "                Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "                neum_term1 = rolling_window_array.T.dot(A_cp) + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash, kron_product, Xi_rep)\n",
    "                denom_term1 = (V_int.dot(A_cp.T)).dot(A_cp) + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "                V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "        print(time_of_day, time.time() - a, calculate_rmse(rolling_window_array, A_cp.dot(V_int.T)))\n",
    "        V_initialize_previous = V_int.copy()\n",
    "        V = (cp.asarray(mag_A)).dot(V_int.T).T/data_max\n",
    "        xTrain1 = V[:, :]\n",
    "        xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "        if flag == 1:\n",
    "            xTrain1final = np.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "        elif flag == 0:\n",
    "            xTrain1final = xTrain1.copy()\n",
    "        \n",
    "        for iteration in range(0, 40):\n",
    "            if iteration == 0:\n",
    "                rolling_window_array = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day + future_time_steps])\n",
    "                V_int = cp.random.rand(rolling_window_array.shape[1],r)\n",
    "                V_int[0:rolling_window, ] = V_initialize_previous\n",
    "                V_int_prev = V_int.copy()\n",
    "                W_temp = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "                W_temp = W_temp[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "                Deg_temp = Deg[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "                Deg_temp = Deg_temp[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "                kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "                Weight_prev = (V_int + 0.0001)/(V_int + 0.0001)\n",
    "                Weight_prev[time_of_day:] = 0\n",
    "                \n",
    "            if iteration >=0:\n",
    "                Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "                neum_term1 = (rolling_window_array.T.dot(A_cp) \n",
    "                              + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash2, kron_product, Xi_rep)\n",
    "                             + hy_prev * Weight_prev * V_int_prev)\n",
    "                denom_term1 = ((V_int.dot(A_cp.T)).dot(A_cp) \n",
    "                               + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "                              + hy_prev * Weight_prev * V_int)\n",
    "                V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "        V = (cp.asarray(mag_A)).dot(V_int.T).T / data_max\n",
    "        xPred1 = V[:, :]\n",
    "        xPred1 = xPred1.reshape(1, *xPred1.shape)\n",
    "        if flag == 1:\n",
    "            yTrain1final = np.concatenate((yTrain1final, xPred1), axis=0)\n",
    "        elif flag == 0:\n",
    "            yTrain1final = xPred1.copy()\n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8266856-bf15-407a-8a1d-debc466109a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Khatri_Rao_Product(A1, B1):\n",
    "    result_list = [i * A1.shape[1] + i for i in range(B1.shape[1])]\n",
    "    temp = csr_matrix(kron(A1, B1))\n",
    "    temp = temp[:,result_list ]\n",
    "    return(temp)\n",
    "def vec_to_Mat(vec, shape):\n",
    "    x = shape[0]\n",
    "    y = shape[1]\n",
    "    Mat = vec.reshape(y,x).T\n",
    "    return (Mat)\n",
    "def derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash, kron_product, Xi_rep):\n",
    "    Xi = Xi_rep.T.dot(kron_product).dot(Xi_rep)\n",
    "    derivative_B = (Xi + Xi.T).dot(vect(V_int))\n",
    "    derivative_B_mat = vec_to_Mat(derivative_B, V_int.shape)\n",
    "    return(csr_matrix(derivative_B_mat))\n",
    "def derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep):\n",
    "    Xi = Xi_rep.T.dot(kron(In.T , Deg_temp)).dot(Xi_rep)\n",
    "    derivative_B = (Xi + Xi.T).dot(vect(V_int))\n",
    "    derivative_B_mat = vec_to_Mat(derivative_B, V_int.shape)\n",
    "    return(csr_matrix(derivative_B_mat))\n",
    "\n",
    "def generate_graph_functions_outside_loop(A_cp, length, future_time_steps):\n",
    "    scipy_csr_identity = identity(A_cp.shape[0] * length, format='csr')\n",
    "    I_dash = cp.sparse.csr_matrix(scipy_csr_identity)\n",
    "    In = csr_matrix(cp.identity(A_cp.shape[1]))\n",
    "    scipy_csr_identity2 = identity(A.shape[0] * (length + future_time_steps), format='csr')\n",
    "    I_dash2 = cp.sparse.csr_matrix(scipy_csr_identity2)\n",
    "    return (In, I_dash, I_dash2)\n",
    "\n",
    "def generate_graph_functions_inside_loop_once(In, W_temp):\n",
    "    kron_product = kron(In.T , W_temp)\n",
    "    return(kron_product)\n",
    "\n",
    "def generate_graph_functions_inside_loop(V_int, In, A_cp):\n",
    "    Ip = csr_matrix(cp.identity(V_int.shape[0]))\n",
    "    Xi_rep = kron(Khatri_Rao_Product(In, A_cp), Ip)\n",
    "    return (Ip, Xi_rep)\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    squared_diff = (y_true - y_pred) ** 2\n",
    "    mean_squared_diff = cp.mean(squared_diff)\n",
    "    rmse = cp.sqrt(mean_squared_diff)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf46a9b-ce35-4055-8f56-12af67d2d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import identity\n",
    "from cupyx.scipy.sparse import kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b15c433f-0c3e-465f-8a61-e9bece47d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "# Set up data directory\n",
    "name = 'Logan'\n",
    "data_directory = r'\\\\hpc-fs.qut.edu.au\\n10680535\\Paper_5\\Datasets'\n",
    "\n",
    "# Load data\n",
    "Tens = np.load(os.path.join(data_directory, f'Train_{name}_3D_data.npy'))\n",
    "Test_Tens = np.load(os.path.join(data_directory, f'Test_{name}_3D_data.npy'))\n",
    "test_dataset = pd.read_csv(os.path.join(data_directory, f'test_{name}_data.csv'), index_col=[0, 1], header=[0])\n",
    "Val_Tens = np.load(os.path.join(data_directory, f'Val_{name}_3D_data.npy'))\n",
    "val_dataset = pd.read_csv(os.path.join(data_directory, f'Val_{name}_data.csv'), index_col=[0, 1], header=[0])\n",
    "ind = pd.read_csv(os.path.join(data_directory, 'date_time_train.csv'), index_col=[0, 1])\n",
    "stl_clm = pd.read_csv(os.path.join(data_directory, 'section_names.csv'), index_col=0)\n",
    "train_data_shubham = pd.read_csv(os.path.join(data_directory, f'Train_{name}_data.csv'), index_col=[0, 1], header=[0])\n",
    "W = sparse.load_npz(os.path.join(data_directory, f'Dynamic_Adjacency_{name}.npz'))\n",
    "MWY_Sections = pd.read_csv(os.path.join(data_directory, 'MWY_Sections.csv'))\n",
    "\n",
    "# Extract section IDs\n",
    "section_ids = train_data_shubham.columns.to_list()\n",
    "serial_numbers = [test_dataset.columns.get_loc(section_id) for section_id in section_ids]\n",
    "\n",
    "# Manipulate data based on section IDs\n",
    "Tens = Tens[serial_numbers, :, :]\n",
    "Test_Tens = Test_Tens[serial_numbers, :, :]\n",
    "Val_Tens = Val_Tens[serial_numbers, :, :]\n",
    "stl_clm = stl_clm.iloc[serial_numbers]\n",
    "train_data_shubham = train_data_shubham.T.iloc[serial_numbers].T\n",
    "test_dataset = test_dataset.T.iloc[serial_numbers].T\n",
    "val_dataset = val_dataset.T.iloc[serial_numbers].T\n",
    "\n",
    "# Additional data manipulations\n",
    "stl_clm.index = stl_clm.index.astype('str')\n",
    "test_dataset = test_dataset[stl_clm.index].T\n",
    "test_data_prior = test_dataset.copy(deep=True)\n",
    "val_dataset = val_dataset[stl_clm.index].T\n",
    "val_data_prior = val_dataset.copy(deep=True)\n",
    "test_dataset_col = test_dataset.columns\n",
    "val_dataset_col = val_dataset.columns\n",
    "train_data_shubham = train_data_shubham[test_data_prior.index.astype(str)]\n",
    "train_data_shubham_prior = train_data_shubham.T\n",
    "\n",
    "# Update serial numbers for W\n",
    "serial_numbers_W = serial_numbers.copy()\n",
    "for i in range(1, 96):\n",
    "    for j in serial_numbers:\n",
    "        serial_numbers_W.append(i * 476 + j)\n",
    "\n",
    "# Update adjacency matrix W\n",
    "W = coo_matrix(W.toarray()[serial_numbers_W].T[serial_numbers_W].T)\n",
    "W = coo_matrix(W)\n",
    "W_dense = W.toarray()\n",
    "W = (W + W.T) / 2\n",
    "W.setdiag(0, k=0)\n",
    "Deg = sparse.diags(np.squeeze(np.asarray(W.sum(axis=0)))).tocsr()\n",
    "\n",
    "# File path for complete data\n",
    "file_path = os.path.join(data_directory, 'Complete_Logan_3D_data.npy')\n",
    "Complete_Tens = np.load(file_path)\n",
    "Complete_Tens = Complete_Tens[serial_numbers, :, :]\n",
    "\n",
    "# Additional data transformation\n",
    "Tens1 = Tens.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f85d2e-a8b6-4abc-a5e7-0f5c297782ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = r'\\\\hpc-fs.qut.edu.au\\n10680535\\Paper_5\\5'\n",
    "A = np.load('A.npy')\n",
    "B = np.load('B.npy')\n",
    "C = np.load('C.npy')\n",
    "A.shape\n",
    "flag = 0\n",
    "rolling_window = 12\n",
    "future_time_steps = 4\n",
    "last_time = 88\n",
    "hyper_smooth = 0\n",
    "weight_future = 0.001\n",
    "hyp_x =hyp_previous_reg= 2000\n",
    "weight_current_pred = 20000\n",
    "mag_A = np.diag(np.asarray(np.sqrt((A.T.dot(A)).sum(axis=1))).reshape(-1))\n",
    "mag_A_inverse = np.linalg.inv(mag_A)\n",
    "data = pd.DataFrame((mag_A).dot(linalg.khatri_rao(B, C).T).T, index=ind.index).round(5)\n",
    "data_max = np.array(data).max()\n",
    "rank = r = A.shape[1]\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef342ecc-fb05-45f5-84b6-378e9767ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, B.shape, C.shape, Tens.shape, Tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afc151-674e-46d9-afe6-030f5ea6afcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hy1 = 0.1\n",
    "flag = 0\n",
    "hy_prev = 100\n",
    "from cupyx.scipy.sparse import csr_matrix, diags\n",
    "for day in range(0, 20):\n",
    "    a = time.time()\n",
    "    current_day_OO = Tens[:, day, :].copy()\n",
    "    e = 0.00000001\n",
    "    \n",
    "    for time_of_day in range(rolling_window, last_time):\n",
    "        for iteration in range(0, 25):\n",
    "            A_cp = cp.asarray(A.copy())\n",
    "            if iteration == 0:\n",
    "                rolling_window_array_complete = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day])\n",
    "                V_int = cp.random.rand(rolling_window_array_complete.shape[1],A.shape[1])\n",
    "                W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "                W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "                Deg_temp_complete = Deg[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "                Deg_temp_complete = Deg_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "            random_numbers = sorted([random.randint(0, Tens.shape[0]-1) for _ in range(60)])\n",
    "            list_final = []\n",
    "            for ii in range (0, rolling_window):\n",
    "                for jj in random_numbers:\n",
    "                    list_final.append(Tens.shape[0] * ii + jj)\n",
    "            A_cp = A_cp[random_numbers, :]\n",
    "            In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "            rolling_window_array =  rolling_window_array_complete[random_numbers, :]\n",
    "            W_temp = W_temp_complete[list_final, :]\n",
    "            W_temp = W_temp[:, list_final]\n",
    "            Deg_temp = csr_matrix(cp.diag(cp.sum(W_temp.toarray(), axis=1).ravel()))\n",
    "            #In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "            kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "            if iteration >=0:\n",
    "                Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "                neum_term1 = rolling_window_array.T.dot(A_cp) + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash, kron_product, Xi_rep)\n",
    "                denom_term1 = (V_int.dot(A_cp.T)).dot(A_cp) + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "                V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "        print(time_of_day, time.time() - a, calculate_rmse(rolling_window_array_complete, cp.asarray(A.copy()).dot(V_int.T)))\n",
    "        V_initialize_previous = V_int.copy()\n",
    "        V = (cp.asarray(mag_A)).dot(V_int.T).T/data_max\n",
    "        xTrain1 = V[:, :]\n",
    "        xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "        if flag == 1:\n",
    "            xTrain1final = cp.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "        elif flag == 0:\n",
    "            xTrain1final = xTrain1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22669917-97b4-42c2-9ddf-cd29cff72b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing  \n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a875d60-13c2-4b94-8f92-1eb7c3ae6ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hy1 = 0.1\n",
    "flag = 0\n",
    "hy_prev = 10000\n",
    "from cupyx.scipy.sparse import csr_matrix, diags\n",
    "for day in range(0, 20):\n",
    "    a = time.time()\n",
    "    current_day_OO = Tens[:, day, :].copy()\n",
    "    e = 0.00000001\n",
    "    W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "    W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "    W_temp_complete2 = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "    W_temp_complete2 = W_temp[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "    #W_temp_complete2 = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "    #W_temp_complete2 = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "    rolling_window_array_complete = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day])\n",
    "    V_int = cp.random.rand(rolling_window_array_complete.shape[1],A.shape[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85214245-e303-4b96-8be2-cafa6a0c3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53037d49-4362-4752-8fa7-9417fc9a33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_temp_complete2 = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "W_temp_complete2 = W_temp_complete2[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "W_temp_complete2.shape\n",
    "flag = 0\n",
    "flago = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744219ce-4a2b-4216-a512-f89aa5270155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tods = list(range(20, last_time))\n",
    "hy1 = 0.0001\n",
    "hy_prev = 5000\n",
    "flago = 0\n",
    "hy_smooth = 0\n",
    "from cupyx.scipy.sparse import csr_matrix, diags\n",
    "for day in range(0, 270):\n",
    "    abcd = time.time()\n",
    "    current_day_OO = Tens[:, day, :].copy()\n",
    "    e = 0.00000001\n",
    "    complete_list = []\n",
    "    for indexo , time_of_day in enumerate(tods):\n",
    "        W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "        W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "        W_temp_complete2 = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "        W_temp_complete2 = W_temp_complete2[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "        temp = []\n",
    "        temp.append(time_of_day)\n",
    "        temp.append(A)\n",
    "        temp.append(current_day_OO)\n",
    "        temp.append(W_temp_complete)\n",
    "        temp.append(W_temp_complete2)\n",
    "        temp.append(hy1)\n",
    "        temp.append(e)\n",
    "        temp.append(mag_A)\n",
    "        temp.append(data_max)\n",
    "        temp.append(rolling_window)\n",
    "        temp.append(future_time_steps)\n",
    "        temp.append(hy_prev)\n",
    "        temp.append(indexo)\n",
    "        temp.append(hy_smooth)\n",
    "        complete_list.append(temp)\n",
    "    import defs_np_new74 as defs_np\n",
    "    results_x = []\n",
    "    results_y = []\n",
    "    results_z = []\n",
    "    def collect_results(result):\n",
    "        results_x.append(result[0])\n",
    "        results_y.append(result[1])\n",
    "        results_z.append(result[2])\n",
    "    if __name__ == '__main__':\n",
    "        start_time = time.time()\n",
    "        pool = multiprocessing.Pool(processes = multiprocessing.cpu_count())\n",
    "        print(day)\n",
    "        for i_index, i in enumerate(complete_list):\n",
    "            try:\n",
    "                result = pool.apply_async(defs_np.process_time_of_day, args = [i[0], i[1], i[2],\n",
    "                               i[3], i[4], i[5], \n",
    "                               i[6], i[7], i[8], i[9],i[10], i[11], i[12],i[13], ], callback = collect_results)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Error in worker process: {e}\")\n",
    "        pool.close()\n",
    "        print('OK')\n",
    "        pool.join()\n",
    "        print('OK2')\n",
    "    position_list = [results_z.index(i) for i in range(len(results_z))]\n",
    "    #print(position_list)\n",
    "\n",
    "    if flago == 1:\n",
    "        x = np.concatenate(results_x)[position_list, :, :]\n",
    "        y = np.concatenate(results_y)[position_list, :, :]\n",
    "        xTrain1final = np.concatenate((xTrain1final,x), axis = 0)\n",
    "        yTrain1final = np.concatenate((yTrain1final,y), axis = 0)\n",
    "    if flago == 0:\n",
    "        xTrain1final =np.concatenate(results_x)[position_list, :, :]\n",
    "        yTrain1final = np.concatenate(results_y)[position_list, :, :]\n",
    "        flago = 1\n",
    "    print(day, xTrain1final.shape, yTrain1final.shape, time.time()-abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d6d27-69dc-4297-a704-f8295789c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Example loop to generate plots\n",
    "for i in range(0, 10):\n",
    "    fig, ax = plt.subplots()\n",
    "    y_train_df = pd.DataFrame(xTrain1final[30])\n",
    "    x_train_df = pd.DataFrame(yTrain1final[30])\n",
    "    pd.DataFrame({'V-output sequence': y_train_df[i]}).plot(ax=ax, color='red', label='V-output sequence')\n",
    "    pd.DataFrame({'V-output sequence': x_train_df[i]}).plot(ax=ax, label='V-input sequence')\n",
    "    plt.xlabel('time-period')  # Set x-axis label\n",
    "    plt.ylabel('coefficient')  # Set y-axis label\n",
    "    plt.title('Sequences')  # Set plot title\n",
    "    plt.legend()  # Display legend\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fb872-9657-4cf2-b46f-7432ed4fb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = A.dot(Khatri_Rao_Product(csr_matrix(cp.asarray(B)), csr_matrix(cp.asarray(C))).get().toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48476a7-e3b6-466d-9548-6a4ebce13494",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = pd.DataFrame(rec, index = train_data_shubham.columns, columns = train_data_shubham.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de8e71-0d9b-4b49-8a23-5b756a550c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec177f0-ef59-447d-a38d-a576790b87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the values for rolling_window and future_time_steps\n",
    "rolling_window = 12\n",
    "future_time_steps = 4\n",
    "Index_final, yTrain1true_Original1 = Generate_original_sequence_arrays(Tens, rolling_window = 20, \n",
    "                                                                       future_time_steps = 4,\n",
    "                                                                       max_days = 4, \n",
    "                                                                       last_time = 88)\n",
    "yTrain1true_Original1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f4963-e34a-483f-ad04-e9970f0a6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain1final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca7eb4-464c-4831-b93e-33ee4e7ff821",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(yTrain1true_Original1[:, 0, :].get())[90][0:68 * 1].plot()\n",
    "pd.DataFrame(A.dot(mag_A_inverse.dot(yTrain1final[:, 12, :].T)) * data_max).T[90][0:68 * 1].plot(color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b94cc9-abb4-4251-b137-c63806609e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de359d30-ff2b-4fbe-9915-a4ad20bd4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flago1 = 0\n",
    "rolling_window = 12\n",
    "future_time_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cdd78-8d96-443d-a950-81cc96d19ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tods = list(range(20, last_time))\n",
    "from cupyx.scipy.sparse import csr_matrix, diags\n",
    "for day in range(0, 30):\n",
    "    abcd = time.time()\n",
    "    current_day_OO = Test_Tens[:, day, :].copy()\n",
    "    e = 0.00000001\n",
    "    complete_list = []\n",
    "    for indexo , time_of_day in enumerate(tods):\n",
    "        W_temp_complete = W[:, (time_of_day - rolling_window)* Test_Tens.shape[0]: time_of_day* Test_Tens.shape[0]]\n",
    "        W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Test_Tens.shape[0]: time_of_day* Test_Tens.shape[0], :]\n",
    "        W_temp_complete2 = W[:, (time_of_day - rolling_window)* Test_Tens.shape[0]: (time_of_day +future_time_steps) * Test_Tens.shape[0]]\n",
    "        W_temp_complete2 = W_temp_complete2[(time_of_day - rolling_window)* Test_Tens.shape[0]: (time_of_day +future_time_steps) * Test_Tens.shape[0], :]\n",
    "        temp = []\n",
    "        temp.append(time_of_day)\n",
    "        temp.append(A)\n",
    "        temp.append(current_day_OO)\n",
    "        temp.append(W_temp_complete)\n",
    "        temp.append(W_temp_complete2)\n",
    "        temp.append(hy1)\n",
    "        temp.append(e)\n",
    "        temp.append(mag_A)\n",
    "        temp.append(data_max)\n",
    "        temp.append(rolling_window)\n",
    "        temp.append(future_time_steps)\n",
    "        temp.append(hy_prev)\n",
    "        temp.append(indexo)\n",
    "        temp.append(hy_smooth)\n",
    "        complete_list.append(temp)\n",
    "    results_x = []\n",
    "    results_y = []\n",
    "    results_z = []\n",
    "    def collect_results(result):\n",
    "        results_x.append(result[0])\n",
    "        results_y.append(result[1])\n",
    "        results_z.append(result[2])\n",
    "    if __name__ == '__main__':\n",
    "        start_time = time.time()\n",
    "        pool = multiprocessing.Pool(processes = multiprocessing.cpu_count() - 4)\n",
    "        print(day)\n",
    "        for i_index, i in enumerate(complete_list):\n",
    "            try:\n",
    "                result = pool.apply_async(defs_np.process_time_of_day, args = [i[0], i[1], i[2],\n",
    "                               i[3], i[4], i[5], \n",
    "                               i[6], i[7], i[8], i[9],i[10], i[11], i[12],i[13], ], callback = collect_results)\n",
    "            except:\n",
    "                print(e)\n",
    "                print(f\"Error in worker process: {e}\")\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    position_list = [results_z.index(i) for i in range(len(results_z))]\n",
    "    #print(position_list)\n",
    "    if flago1 == 1:\n",
    "        x = np.concatenate(results_x)[position_list, :, :]\n",
    "        y = np.concatenate(results_y)[position_list, :, :]\n",
    "        xVal1final = np.concatenate((xVal1final,x), axis = 0)\n",
    "        yVal1final = np.concatenate((yVal1final,y), axis = 0)\n",
    "    if flago1 == 0:\n",
    "        xVal1final = np.concatenate(results_x)[position_list, :, :]\n",
    "        yVal1final = np.concatenate(results_y)[position_list, :, :]\n",
    "        flago1 = 1\n",
    "    print(day, xVal1final.shape, yVal1final.shape, time.time()-abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbf677-0240-4226-9e77-a597cce1ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yVal1final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf42dc-0c9a-40a8-ab47-6df67d1fe172",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('yTrain1final_g2.npy', yTrain1final)\n",
    "np.save('xTrain1final_g2.npy', xTrain1final)\n",
    "np.save('yVal1final_g1.npy', yVal1final)\n",
    "np.save('xVal1final_g1.npy', xVal1final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1e8cc-22da-42bd-b105-5bddcf50430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "defs_np.process_time_of_day(i[0], i[1], i[2],\n",
    "                           i[3], i[4], i[5], \n",
    "                           i[6], i[7], i[8], i[9],i[10], i[11],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0ab8e-ebe8-4cd3-b113-55f2340041b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bca7f-a106-4d33-aaac-9c1f98cd299a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_x = []\n",
    "results_y = []\n",
    "def collect_results(result):\n",
    "    results_x.append(result[0])\n",
    "    results_y.append(result[1])\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count() -5)\n",
    "    for i in complete_list:\n",
    "        try:\n",
    "            result = pool.apply_async(defs_np020.process_time_of_day, args = [i[0], i[1], i[2],\n",
    "                           i[3], i[4], i[5], \n",
    "                           i[6], i[7], i[8], i[9],i[10], i[11],], callback = collect_results)\n",
    "            print('yes')\n",
    "        except:\n",
    "            print(f\"Error in worker process: {e}\")\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841bca5-54c9-4369-aa1a-b9ce4979ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d5611-b448-46fd-8140-0410165efe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in complete_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e178a75-7aac-4f75-9c85-49221d43db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import defs_np021\n",
    "def collect_results(result):\n",
    "    results.append( result)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849c6c7-9a6f-498b-8b28-29081488f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = complete_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c057794-ccb3-4423-a70e-ac73870e2ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atb = defs_np.process_time_of_day(i[0], i[1], i[2],\n",
    "                               i[3], i[4], i[5], \n",
    "                               i[6], i[7], i[8], i[9],i[10], i[11], i[12],i[13],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdf321-285a-42a2-ac34-584e0139de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atb[0].shape, atb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a3f54-bc81-49a3-94d6-8374d79e89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import defs_np019\n",
    "results_x = []\n",
    "results_y = []\n",
    "def collect_results(result):\n",
    "    results_x.append(result[0])\n",
    "    results_y.append(result[1])\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count() -5)\n",
    "    print('yes')\n",
    "    for i in complete_list:\n",
    "        try:\n",
    "            result = pool.apply_async(defs_np019.process_time_of_day, args = [i[0], i[1], i[2],\n",
    "                           i[3], i[4], i[5], \n",
    "                           i[6], i[7], i[8], i[9],i[10], i[11],], callback = collect_results)\n",
    "            print('yes')\n",
    "        except:\n",
    "            print(f\"Error in worker process: {e}\")\n",
    "        print('O.K.')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "xTrain1final = np.concatenate(results_x)\n",
    "yTrain1final = np.concatenate(results_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef10a9d-7428-4e01-8cf8-560258b958f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    pool = multiprocessing.Pool(processes = multiprocessing.cpu_count() -5)\n",
    "    print('yes')\n",
    "    for i in complete_list:\n",
    "        try:\n",
    "            result = pool.apply_async(defs_np019.process_time_of_day, args = [i[0], i[1], i[2],\n",
    "                           i[3], i[4], i[5], \n",
    "                           i[6], i[7], i[8], i[9],i[10], i[11],], callback = collect_results)\n",
    "            print('yes')\n",
    "        except:\n",
    "            print(f\"Error in worker process: {e}\")\n",
    "        print('O.K.')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "xTrain1final = np.concatenate(results_x)\n",
    "yTrain1final = np.concatenate(results_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab6f0c-1856-4ab9-ba35-5f5d3c59e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain1final = np.concatenate(results_x)\n",
    "yTrain1final = np.concatenate(results_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bb18a-afbc-45bb-aa0c-48026f8e2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(np.sum(W_temp_complete.toarray(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f7b51-980f-43f8-951d-7e9afa3e22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_of_day(time_of_day):\n",
    "    for iteration in range(0, 25):\n",
    "        A_cp = cp.asarray(A.copy())\n",
    "        if iteration == 0:\n",
    "            rolling_window_array_complete = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day])\n",
    "            V_int = cp.random.rand(rolling_window_array_complete.shape[1],A.shape[1])\n",
    "            W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "            W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "            Deg_temp_complete = Deg[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "            Deg_temp_complete = Deg_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "        random_numbers = sorted([random.randint(0, Tens.shape[0]-1) for _ in range(60)])\n",
    "        list_final = []\n",
    "        for ii in range (0, rolling_window):\n",
    "            for jj in random_numbers:\n",
    "                list_final.append(Tens.shape[0] * ii + jj)\n",
    "        A_cp = A_cp[random_numbers, :]\n",
    "        In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "        rolling_window_array =  rolling_window_array_complete[random_numbers, :]\n",
    "        W_temp = W_temp_complete[list_final, :]\n",
    "        W_temp = W_temp[:, list_final]\n",
    "        Deg_temp = csr_matrix(cp.diag(cp.sum(W_temp.toarray(), axis=1).ravel()))\n",
    "        #In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "        kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "        if iteration >=0:\n",
    "            Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "            neum_term1 = rolling_window_array.T.dot(A_cp) + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash, kron_product, Xi_rep)\n",
    "            denom_term1 = (V_int.dot(A_cp.T)).dot(A_cp) + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "            V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "    print(time_of_day, time.time() - a, calculate_rmse(rolling_window_array_complete, cp.asarray(A.copy()).dot(V_int.T)))\n",
    "    V_initialize_previous = V_int.copy()\n",
    "    V = (cp.asarray(mag_A)).dot(V_int.T).T/data_max\n",
    "    xTrain1 = V[:, :]\n",
    "    xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "    if flag == 1:\n",
    "        xTrain1final = cp.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "    elif flag == 0:\n",
    "        xTrain1final = xTrain1.copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072d9bf-2540-40ec-b8c9-aa59ec060605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape():\n",
    "    while not stop_event.is_set():\n",
    "        print(\"Shape of xTrain1final:\", xTrain1final.shape)\n",
    "        time.sleep(10)\n",
    "        \n",
    "def process_time_of_day(time_of_day):\n",
    "    flag = 0\n",
    "    for iteration in range(0, 25):\n",
    "        A_cp = cp.asarray(A.copy())\n",
    "        if iteration == 0:\n",
    "            rolling_window_array_complete = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day])\n",
    "            V_int = cp.random.rand(rolling_window_array_complete.shape[1],A.shape[1])\n",
    "            W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "            W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "            Deg_temp_complete = Deg[:, (time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0]]\n",
    "            Deg_temp_complete = Deg_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: time_of_day* Tens.shape[0], :]\n",
    "        random_numbers = sorted([random.randint(0, Tens.shape[0]-1) for _ in range(60)])\n",
    "        list_final = []\n",
    "        for ii in range (0, rolling_window):\n",
    "            for jj in random_numbers:\n",
    "                list_final.append(Tens.shape[0] * ii + jj)\n",
    "        A_cp = A_cp[random_numbers, :]\n",
    "        In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "        rolling_window_array =  rolling_window_array_complete[random_numbers, :]\n",
    "        W_temp = W_temp_complete[list_final, :]\n",
    "        W_temp = W_temp[:, list_final]\n",
    "        Deg_temp = csr_matrix(cp.diag(cp.sum(W_temp.toarray(), axis=1).ravel()))\n",
    "        #In, I_dash, I_dash2 = generate_graph_functions_outside_loop(A_cp, length = rolling_window, future_time_steps= future_time_steps)\n",
    "        kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "        if iteration >=0:\n",
    "            Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "            neum_term1 = rolling_window_array.T.dot(A_cp) + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash, kron_product, Xi_rep)\n",
    "            denom_term1 = (V_int.dot(A_cp.T)).dot(A_cp) + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "            V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "    print(time_of_day, time.time() - a, calculate_rmse(rolling_window_array_complete, cp.asarray(A.copy()).dot(V_int.T)))\n",
    "    V_initialize_previous = V_int.copy()\n",
    "    V = (cp.asarray(mag_A)).dot(V_int.T).T/data_max\n",
    "    xTrain1 = V[:, :]\n",
    "    xTrain1 = xTrain1.reshape(1, *xTrain1.shape)\n",
    "    if flag == 1:\n",
    "        xTrain1final = np.concatenate((xTrain1final, xTrain1), axis=0)\n",
    "    elif flag == 0:\n",
    "        xTrain1final = xTrain1.copy()\n",
    "    return(xTrain1final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50469756-119f-4898-ada1-d6f2f9d33405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0db88f-a175-4d99-bb0c-2631b5a3be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(0, 40):\n",
    "    A_cp = cp.asarray(A.copy())\n",
    "    if iteration == 0:\n",
    "        rolling_window_array = cp.asarray(current_day_OO[:, time_of_day-rolling_window:time_of_day + future_time_steps])\n",
    "        V_int = cp.random.rand(rolling_window_array.shape[1],r)\n",
    "        V_int[0:rolling_window, ] = V_initialize_previous\n",
    "        V_int_prev = V_int.copy()\n",
    "        W_temp_complete = W[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "        W_temp_complete = W_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "        Deg_temp_complete = Deg_complete[:, (time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0]]\n",
    "        Deg_temp = Deg_temp_complete[(time_of_day - rolling_window)* Tens.shape[0]: (time_of_day +future_time_steps) * Tens.shape[0], :]\n",
    "        kron_product = generate_graph_functions_inside_loop_once(In, W_temp)\n",
    "        Weight_prev = (V_int + 0.0001)/(V_int + 0.0001)\n",
    "        Weight_prev[time_of_day:] = 0\n",
    "    if iteration >=0:\n",
    "        Ip, Xi_rep = generate_graph_functions_inside_loop(V_int, In, A_cp)\n",
    "        neum_term1 = (rolling_window_array.T.dot(A_cp) \n",
    "                      + hy1 * derivative_dynamic_graph_B_W_cp(A_cp, V_int, W_temp, In, Ip, I_dash2, kron_product, Xi_rep)\n",
    "                     + hy_prev * Weight_prev * V_int_prev)\n",
    "        denom_term1 = ((V_int.dot(A_cp.T)).dot(A_cp) \n",
    "                       + hy1 * derivative_dynamic_graph_B_D_cp(A_cp, V_int, Deg_temp, In, Ip, Xi_rep)\n",
    "                      + hy_prev * Weight_prev * V_int)\n",
    "        V_int = V_int * (neum_term1 + e )/ (denom_term1 + e)\n",
    "V = (cp.asarray(mag_A)).dot(V_int.T).T / data_max\n",
    "xPred1 = V[:, :]\n",
    "xPred1 = xPred1.reshape(1, *xPred1.shape)\n",
    "if flag == 1:\n",
    "    yTrain1final = np.concatenate((yTrain1final, xPred1), axis=0)\n",
    "elif flag == 0:\n",
    "    yTrain1final = xPred1.copy()\n",
    "    flag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad22bf-ac5c-412c-b475-df2fd87d3900",
   "metadata": {},
   "source": [
    "10. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad0abf-60c6-4838-a263-cc61e1c3fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "name= 'Logan'\n",
    "Data_directory =os.getcwd()\n",
    "\n",
    "Data_directory = r'\\\\hpc-fs.qut.edu.au\\n10680535\\Paper_5\\Datasets'\n",
    "from scipy import sparse\n",
    "Tens = np.load(Data_directory + os.sep + 'Train_' + name + '_3D_data.npy')\n",
    "Test_Tens = np.load(Data_directory + os.sep + 'Test_' + name + '_3D_data.npy')\n",
    "test_dataset = pd.read_csv(Data_directory + os.sep + 'test_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "Val_Tens = np.load(Data_directory + os.sep + 'Val_' + name + '_3D_data.npy')\n",
    "val_dataset = pd.read_csv(Data_directory + os.sep + 'Val_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "ind = pd.read_csv(Data_directory + os.sep + 'date_time_train.csv', index_col = [0,1])\n",
    "stl_clm = pd.read_csv(Data_directory + os.sep + 'section_names.csv', index_col = 0)\n",
    "train_data_shubham= pd.read_csv(Data_directory + os.sep + r'Train_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "W = sparse.load_npz(Data_directory + os.sep + 'Dynamic_Adjacency_' + name+'.npz')\n",
    "#W = np.load(Data_directory + os.sep + 'Static_Adjacency_'+ name + '_Topo_Traffic.npy')\n",
    "MWY_Sections = pd.read_csv(Data_directory + os.sep + r'MWY_Sections.csv')\n",
    "#section_ids = MWY_Sections['Sections'].tolist()  # Get the list of section IDs'''\n",
    "section_ids = train_data_shubham.columns.to_list()\n",
    "serial_numbers = [test_dataset.columns.get_loc(section_id)  for section_id in section_ids]\n",
    "Tens = Tens[serial_numbers, :, :]\n",
    "Test_Tens = Test_Tens[serial_numbers, :, :]\n",
    "test_dataset = pd.read_csv(Data_directory + os.sep + 'test_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "test_dataset = test_dataset.T.iloc[serial_numbers].T\n",
    "Val_Tens = Val_Tens[serial_numbers, :, :]\n",
    "val_dataset = pd.read_csv(Data_directory + os.sep + 'Val_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "val_dataset = val_dataset.T.iloc[serial_numbers].T\n",
    "stl_clm = stl_clm.iloc[serial_numbers]\n",
    "train_data_shubham = train_data_shubham.T.iloc[serial_numbers].T\n",
    "stl_clm.index = stl_clm.index.astype('str')\n",
    "test_dataset = test_dataset[stl_clm.index].T\n",
    "test_data_prior = test_dataset.copy(deep = True)\n",
    "val_dataset = val_dataset[stl_clm.index].T\n",
    "val_data_prior = val_dataset.copy(deep = True)\n",
    "test_dataset_col =  test_dataset.columns\n",
    "val_dataset_col =  val_dataset.columns\n",
    "train_data_shubham = train_data_shubham[test_data_prior.index.astype(str)]\n",
    "train_data_shubham_prior = train_data_shubham.T\n",
    "serial_numbers_W = serial_numbers.copy()\n",
    "for i in range(1, 96):\n",
    "    for j in serial_numbers:\n",
    "        serial_numbers_W.append(i*476 + j)\n",
    "#W= coo_matrix(W.toarray()[serial_numbers_W].T[serial_numbers_W].T)\n",
    "W = coo_matrix(W)\n",
    "W_dense = W.toarray()\n",
    "'''# Create a heatmap plot of the dense matrix\n",
    "plt.imshow(W_dense, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()  # Add colorbar for reference\n",
    "plt.title(\"Heatmap of W\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()'''\n",
    "W = (W + W.T)/2\n",
    "W.setdiag(0, k=0)\n",
    "Deg = sparse.diags(np.squeeze(np.asarray(W.sum(axis = 0)))).tocsr()\n",
    "W.setdiag(0, k=0)\n",
    "Deg = sparse.diags(np.squeeze(np.asarray(W.sum(axis = 0)))).tocsr()\n",
    "file_path = Data_directory + os.sep + 'Complete_Logan_3D_data.npy'\n",
    "Complete_Tens = np.load(file_path)\n",
    "Complete_Tens = Complete_Tens[serial_numbers, :, :]\n",
    "Tens1 = Tens.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6e95d-bacf-42b6-a57d-4b48dcab14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = r'\\\\hpc-fs.qut.edu.au\\n10680535\\Paper_5\\5'\n",
    "A = np.load('A.npy')\n",
    "B = np.load('B.npy')\n",
    "C = np.load('C.npy')\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981ae98-b25d-4aeb-bd67-d19c25de7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df = pd.DataFrame(A.dot(linalg.khatri_rao(B, C).T), \n",
    "                      index = train_data_shubham.columns, columns = train_data_shubham.index).T\n",
    "train_data_shubham_prior.T['11780201_st0'].iloc[0:100].plot()\n",
    "rec_df['11780201_st0'].iloc[0:100].plot(color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18b808-7f6e-484b-9c29-ce3125ec21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_A = np.diag(np.asarray(np.sqrt((A.T.dot(A)).sum(axis=1))).reshape(-1))\n",
    "mag_A_inverse = np.linalg.inv(mag_A)\n",
    "data = pd.DataFrame((mag_A).dot(linalg.khatri_rao(B, C).T).T, index=ind.index).round(5)\n",
    "data_max = np.array(data).max()\n",
    "rank = r = A.shape[1]\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c71757-d9d2-456e-8b6b-93869fbf63ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2: Pattern mining\n",
    "Patterns_B, Patterns = generate_patterns_and_factors(train_data_shubham_prior, B, rank=rank, alpha=0, alpha2=0.1, e=1E-10, n_iter=500, eps=2000, MinPts=4, ssims=[0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b73f74-f8a4-4d1a-9b9f-d9adaedbf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "rolling_window = 20\n",
    "future_time_steps = 4\n",
    "last_time = 88\n",
    "hyper_smooth = 0\n",
    "weight_future = 0.001\n",
    "hyp_x =hyp_previous_reg= 2000\n",
    "weight_current_pred = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde0775-0291-4be2-b082-c966320f2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Offline_encoding_graph_stochastic(Tens, train_data_shubham, A, B, C, Deg, W, ind,rolling_window, future_time_steps, hyper_smooth, \n",
    "                      hyp_previous_reg , weight_future, weight_current_pred,last_time = 88,\n",
    "                      yTrain1final = None, xTrain1final = None, day_max = 20, hy1 = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bf217-fc34-48fe-8053-09dbe03d7fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yTrain1final_g2, xTrain1final_g2 = Offline_encoding(Tens, train_data_shubham, A, B, C, \n",
    "                                                    ind,rolling_window, future_time_steps,\n",
    "                                                    hyper_smooth, hyp_previous_reg,\n",
    "                                                    weight_future, weight_current_pred,\n",
    "                                                    last_time = 88, yTrain1final = None, \n",
    "                                                    xTrain1final = None, day_max = 270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f8ff7-0359-431c-b80f-727440bf7a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yTrain1final_g2, xTrain1final_g2 = Offline_encoding(Tens, train_data_shubham, \n",
    "                                                    A, B, C, \n",
    "                                                    ind, rolling_window, \n",
    "                                                    future_time_steps, \n",
    "                                                    hyper_smooth, hyp_previous_reg ,\n",
    "                                                    weight_future, \n",
    "                                                    weight_current_pred, last_time = 88,\n",
    "                                                    yTrain1final = yTrain1final_g2, \n",
    "                                                    xTrain1final = xTrain1final_g2, \n",
    "                                                    day_max = 270, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1997b2-86f7-4a3c-8ba2-66e54c962fbf",
   "metadata": {},
   "source": [
    "np.save('yTrain1final_g.npy', yTrain1final_g)\n",
    "np.save('xTrain1final_g.npy', xTrain1final_g)\n",
    "np.save('yVal1final_g.npy', yTrain1final_g)\n",
    "np.save('xVal1final_g.npy', xTrain1final_g)\n",
    "np.save('A.npy', A)\n",
    "np.save('B.npy', B)\n",
    "np.save('C.npy', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90833e59-9499-4b02-9640-a3e65bba2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_ind = test_data_prior.copy(deep = True).T\n",
    "val_data_ind['a'] = 0\n",
    "val_data_ind = val_data_ind[['a']]\n",
    "val_data_ind, Val_Tens.shape\n",
    "yVal1final_g1, xVal1final_g1 = Offline_encoding (Test_Tens,test_data_prior.T, A, B, C, \n",
    "                                                 val_data_ind,rolling_window, future_time_steps, \n",
    "                                                 hyper_smooth, hyp_previous_reg , weight_future, \n",
    "                                                 weight_current_pred,last_time = 88,\n",
    "                                                 yTrain1final = None, xTrain1final = None, day_max = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0157ca-d9ac-4f03-af86-32362a842da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yVal1final_g1, xVal1final_g1 = Offline_encoding (Test_Tens,test_data_prior.T, A, B, C, \n",
    "                                                 val_data_ind,rolling_window, future_time_steps, \n",
    "                                                 hyper_smooth, hyp_previous_reg , weight_future,\n",
    "                                                 weight_current_pred,last_time = 88, yTrain1final = yVal1final_g1, \n",
    "                                                 xTrain1final = xVal1final_g1, day_max = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd1aa4-9d46-46e0-8289-77b46bfc6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('yTrain1final_g2.npy', yTrain1final_g2)\n",
    "np.save('xTrain1final_g2.npy', xTrain1final_g2)\n",
    "np.save('yVal1final_g1.npy', yVal1final_g1)\n",
    "np.save('xVal1final_g1.npy', xVal1final_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aeec94-cbe4-4234-993c-8825bb8e8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the values for rolling_window and future_time_steps\n",
    "rolling_window = 12\n",
    "future_time_steps = 4\n",
    "Index_final, yTrain1true_Original1 = Generate_original_sequence_arrays(Tens, rolling_window = 12, \n",
    "                                                                       future_time_steps = 4,\n",
    "                                                                       max_days = 270, \n",
    "                                                                       last_time = 88)\n",
    "yTrain1true_Original1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e76d31-495b-43b8-9afe-0fc1ff34bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Index_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee4177-169f-4d30-b08d-239368494d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the values for rolling_window and future_time_steps\n",
    "rolling_window = 12\n",
    "future_time_steps = 4\n",
    "Index_final_val, yVal1true_Original1 = Generate_original_sequence_arrays(Test_Tens, \n",
    "                                                                         rolling_window = 12,\n",
    "                                                                         future_time_steps = 4, \n",
    "                                                                         max_days = 30, last_time = 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983a2aa-4e65-4145-ab62-cff080f180d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yVal1true_Original1.shape,yTrain1true_Original1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e5494-4281-4d43-b352-84c96e9c38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Index_final.npy', Index_final)\n",
    "np.save('yTrain1true_Original1.npy', yTrain1true_Original1)\n",
    "np.save('Index_final_val.npy', Index_final_val)\n",
    "np.save('yVal1true_Original1.npy', yVal1true_Original1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fbf3a-ded1-4702-b66e-6ac9145f0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example loop to generate plots\n",
    "for i in range(0, 10):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    y_train_df = pd.DataFrame(xTrain1final[20])\n",
    "    x_train_df = pd.DataFrame(yTrain1final[20])\n",
    "\n",
    "    pd.DataFrame({'V-output sequence': y_train_df[i]}).plot(ax=ax, color='red', label='V-output sequence')\n",
    "    pd.DataFrame({'V-output sequence': x_train_df[i]}).plot(ax=ax, label='V-input sequence')\n",
    "    \n",
    "    plt.xlabel('time-period')  # Set x-axis label\n",
    "    plt.ylabel('coefficient')  # Set y-axis label\n",
    "    plt.title('Sequences')  # Set plot title\n",
    "    plt.legend()  # Display legend\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede4c77-bc2d-44cc-a9e9-bf7536c38e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain1final_g2.shape, len(Index_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99062c-7495-48fb-83a2-b597f80eef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
