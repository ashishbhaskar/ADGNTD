{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d618f3-fe60-4e2c-a68a-e0bf86ef04df",
   "metadata": {},
   "source": [
    "1. Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5b252b-b734-41f0-92cc-bb462fda2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import bmat, coo_matrix, csr_matrix, diags, kron, load_npz\n",
    "from scipy import linalg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, TimeDistributed, Flatten, Reshape\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tabulate import tabulate\n",
    "from itertools import chain\n",
    "from termcolor import colored\n",
    "from random import randrange\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp\n",
    "import time\n",
    "import traceback\n",
    "import datetime\n",
    "import math\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb1be2-fada-4531-9253-dadd40a63374",
   "metadata": {},
   "source": [
    "2. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42bb5b7-25cc-406b-a1a9-5bcdaa82f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is using the GPU.\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.random.rand(100)\n",
    "cupy_array = cp.asarray(numpy_array)\n",
    "if cupy_array.data.device.id >= 0:\n",
    "    print(\"CuPy is using the GPU.\")\n",
    "else:\n",
    "    print(\"CuPy is NOT using the GPU.\")\n",
    "\n",
    "def unfolding_3D(Tens, unfol_dim, other_dim_seq ):\n",
    "    X = Tens.transpose(unfol_dim, other_dim_seq[0], other_dim_seq[1]).reshape(Tens.shape[unfol_dim], Tens.shape[other_dim_seq[0]] * Tens.shape[other_dim_seq[1]])\n",
    "    return (X)\n",
    "def folding_3D(Unfold_Tens, unfol_dim , other_dim_seq, Tens_shape):\n",
    "    a = [0,1,2]\n",
    "    items = deque(a)\n",
    "    items.rotate(unfol_dim) \n",
    "    a_up= list(items)\n",
    "    X = Unfold_Tens.reshape(Tens_shape[unfol_dim], Tens_shape[other_dim_seq[0]],Tens_shape[other_dim_seq[1]]).transpose(a_up[0], a_up[1], a_up[2])    \n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ad0abf-60c6-4838-a263-cc61e1c3fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "name= 'Logan'\n",
    "Data_directory =os.getcwd()\n",
    "\n",
    "#Data_directory = r'\\\\hpc-fs.qut.edu.au\\n10680535\\Paper_5\\Datasets'\n",
    "from scipy import sparse\n",
    "Tens = np.load(Data_directory + os.sep + 'Train_' + name + '_3D_data.npy')\n",
    "Test_Tens = np.load(Data_directory + os.sep + 'Test_' + name + '_3D_data.npy')\n",
    "test_dataset = pd.read_csv(Data_directory + os.sep + 'test_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "Val_Tens = np.load(Data_directory + os.sep + 'Val_' + name + '_3D_data.npy')\n",
    "val_dataset = pd.read_csv(Data_directory + os.sep + 'Val_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "ind = pd.read_csv(Data_directory + os.sep + 'date_time_train.csv', index_col = [0,1])\n",
    "stl_clm = pd.read_csv(Data_directory + os.sep + 'section_names.csv', index_col = 0)\n",
    "train_data_shubham= pd.read_csv(Data_directory + os.sep + r'Train_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "W = sparse.load_npz(Data_directory + os.sep + 'W'+ '.npz')\n",
    "#W = np.load(Data_directory + os.sep + 'Static_Adjacency_'+ name + '_Topo_Traffic.npy')\n",
    "MWY_Sections = pd.read_csv(Data_directory + os.sep + r'MWY_Sections.csv')\n",
    "section_ids = MWY_Sections['Sections'].tolist()  # Get the list of section IDs'''\n",
    "section_ids = train_data_shubham.columns.to_list()\n",
    "serial_numbers = [test_dataset.columns.get_loc(section_id)  for section_id in section_ids]\n",
    "Tens = Tens[serial_numbers, :, :]\n",
    "Test_Tens = Test_Tens[serial_numbers, :, :]\n",
    "test_dataset = pd.read_csv(Data_directory + os.sep + 'test_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "test_dataset = test_dataset.T.iloc[serial_numbers].T\n",
    "Val_Tens = Val_Tens[serial_numbers, :, :]\n",
    "val_dataset = pd.read_csv(Data_directory + os.sep + 'Val_' + name + '_data.csv', index_col = [0,1], header = [0])\n",
    "val_dataset = val_dataset.T.iloc[serial_numbers].T\n",
    "stl_clm = stl_clm.iloc[serial_numbers]\n",
    "train_data_shubham = train_data_shubham.T.iloc[serial_numbers].T\n",
    "stl_clm.index = stl_clm.index.astype('str')\n",
    "test_dataset = test_dataset[stl_clm.index].T\n",
    "test_data_prior = test_dataset.copy(deep = True)\n",
    "val_dataset = val_dataset[stl_clm.index].T\n",
    "val_data_prior = val_dataset.copy(deep = True)\n",
    "test_dataset_col =  test_dataset.columns\n",
    "val_dataset_col =  val_dataset.columns\n",
    "train_data_shubham = train_data_shubham[test_data_prior.index.astype(str)]\n",
    "train_data_shubham_prior = train_data_shubham.T\n",
    "serial_numbers_W = serial_numbers.copy()\n",
    "for i in range(1, 96):\n",
    "    for j in serial_numbers:\n",
    "        serial_numbers_W.append(i*476 + j)\n",
    "#W= coo_matrix(W.toarray()[serial_numbers_W].T[serial_numbers_W].T)\n",
    "W = coo_matrix(W)\n",
    "W_dense = W.toarray()\n",
    "'''# Create a heatmap plot of the dense matrix\n",
    "plt.imshow(W_dense, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()  # Add colorbar for reference\n",
    "plt.title(\"Heatmap of W\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()'''\n",
    "W = (W + W.T)/2\n",
    "W.setdiag(0, k=0)\n",
    "Deg = sparse.diags(np.squeeze(np.asarray(W.sum(axis = 0)))).tocsr()\n",
    "W.setdiag(0, k=0)\n",
    "Deg = sparse.diags(np.squeeze(np.asarray(W.sum(axis = 0)))).tocsr()\n",
    "file_path = Data_directory + os.sep + 'Complete_Logan_3D_data.npy'\n",
    "Complete_Tens = np.load(file_path)\n",
    "Complete_Tens = Complete_Tens[serial_numbers, :, :]\n",
    "Tens1 = Tens.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a31db9-81bf-4e8b-af81-b7d8b934dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention layer\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return tf.keras.backend.get_value(optimizer.lr)\n",
    "    return lr\n",
    "\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)      \n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3eed6f-7885-4203-865c-b6c616b727fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.load('A.npy')\n",
    "B = np.load('B.npy')\n",
    "C = np.load('C.npy')\n",
    "rank = r  = A.shape[1]\n",
    "mag_A = np.diag(np.asarray(np.sqrt((A.T.dot(A)).sum(axis=1))).reshape(-1))\n",
    "mag_A_inverse = np.linalg.inv(mag_A)\n",
    "data = pd.DataFrame((mag_A).dot(linalg.khatri_rao(B, C).T).T, index=ind.index).round(5)\n",
    "data_max = np.array(data).max()\n",
    "rank = r = A.shape[1]\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a115bcd-cdcc-4fab-b9d4-66172a653def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as k\n",
    "rolling_window1 = 12\n",
    "forecast_window = 4\n",
    "daily_points = 96\n",
    "lstm_units = 100\n",
    "#rnn_units = 400\n",
    "lstm_act = 'elu'\n",
    "rnn_act = 'elu'\n",
    "return_seq = True\n",
    "lstm_kernel_type= rnn_kernel_type = 'random_normal'\n",
    "bi_layers = 2\n",
    "dense_units = rank\n",
    "dense_act = 'elu'\n",
    "dropout_percent = 0.5\n",
    "batchs = 68 * 100\n",
    "Epochs = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93fa139-abe6-4417-aa46-516704037864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "optim = Adam(learning_rate=.005)\n",
    "lr_metric = get_lr_metric(optim)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=500)\n",
    "cb = TimingCallback()\n",
    "clbcks = [es,cb, PlotLossesKeras(from_step=1)]\n",
    "loss_metrics = [RootMeanSquaredError(name='rmse'),MeanAbsolutePercentageError(name='mape'),get_lr_metric(optim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa2446d-c23d-49f6-a4f2-47d3ff682ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain1final_g2 = cp.load('yTrain1final_g2.npy', )\n",
    "xTrain1final_g2 = cp.load('xTrain1final_g2.npy', )\n",
    "yVal1final_g1 = cp.load('yVal1final_g1.npy', )\n",
    "xVal1final_g1 = cp.load('xVal1final_g1.npy', )\n",
    "Index_final = cp.load('Index_final.npy', )\n",
    "yTrain1true_Original1 = cp.load('yTrain1true_Original1.npy', )\n",
    "Index_final_val = cp.load('Index_final_val.npy', )\n",
    "yVal1true_Original1 = cp.load('yVal1true_Original1.npy', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05cef082-b411-41a2-8526-17ead3f8548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18360, 16, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain1final_g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8582569-2509-4311-83e2-89462ca93929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2040, 16, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yVal1final_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc031209-0f4b-46af-bcbe-bf7cc3472003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yTrain1true_Original1 = yTrain1true_Original1[68:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622285d-94b6-402d-9151-9e70e3adbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain1true_Original1.shape, yVal1final_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba4bf5-8cc9-4c76-a3f5-a2739903496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = list(range(xTrain1final_g2.shape[0]))\n",
    "list2 = list(range(xVal1final_g1.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d81bbf-6fc3-47bc-a773-02705d2a4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56898149-3038-45f6-9add-57d58bce3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Conv1D, LSTM\n",
    "def bilstm_attention(input_shape,bi_layers,optim, lstm_units,lstm_act,\n",
    "                     return_seq,lstm_kernel_type,dropout_percent,\n",
    "                     dense_units,dense_act,loss_metrics, forecast_window, rolling_window1,\n",
    "                     custom_loss3, weights_tf, A_tensor, data_max, mag_A_inverse_tensor):\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    print(\"Shape of input_layer:\", input_layer.shape)\n",
    "    #layer = Dense(dense_units,activation = dense_act)(input_layer)\n",
    "    #layer = Conv1D(filters=32, kernel_size=3)(layer)\n",
    "    layer = TimeDistributed(tf.keras.layers.Flatten())(input_layer)\n",
    "    for i in range(0, bi_layers):\n",
    "        layer = Bidirectional(LSTM(lstm_units,activation=lstm_act,return_sequences=return_seq,\n",
    "                                   kernel_initializer=lstm_kernel_type,))(layer)\n",
    "        print(\"Shape of BILSTM_layer:\", layer.shape)\n",
    "        attention_layer = attention()(layer)\n",
    "        layer = tf.keras.layers.Dropout(dropout_percent)(layer)\n",
    "    #layer 3\n",
    "    flat_layer = Flatten()(layer)\n",
    "    print(\"Shape of flat_layer:\", layer.shape)\n",
    "    reshape_layer = Reshape((forecast_window,2*(lstm_units*(int(rolling_window1/forecast_window)))))(flat_layer)\n",
    "    print(\"Shape of reshaped_layer:\", reshape_layer.shape)\n",
    "    layer = Dense(dense_units,activation = dense_act)(reshape_layer)\n",
    "    output_layer = Dense(input_shape[1], bias_regularizer = l2(0.1), kernel_regularizer = l2(0.0001), )(layer) #\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    #optim = tf.keras.optimizers.Adam(learning_rate=lr_schedule(step),decay = 0,clipnorm=None)\n",
    "    model.compile(loss=custom_loss5(weights_tf = weights_tf, A_tensor= A_tensor, data_max = data_max, mag_A_inverse_tensor = mag_A_inverse_tensor),metrics=loss_metrics, optimizer= optim)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0cbff-fe6b-48dc-809b-446debe3e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(xTrain1final_g2.shape[0]/68) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f059bd9-a44d-427a-bfa9-0da832298809",
   "metadata": {},
   "source": [
    "list1 = []\n",
    "for j in range (0, int(xTrain1final_g2.shape[0]/68)):\n",
    "    '''\n",
    "    for i in range (8, 17):\n",
    "        list1.append(j * 68 + i)\n",
    "    '''\n",
    "    for i in range (42, 52):\n",
    "        list1.append(j * 68 + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675f1ec-9305-4df9-8be3-ed7a6c472623",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list1), xTrain1final_g2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd2982-79da-416b-bf95-af709f16a2bb",
   "metadata": {},
   "source": [
    "list2 = []\n",
    "for j in range (0, int(xVal1final_g1.shape[0]/68)):\n",
    "\n",
    "    for i in range (8, 17):\n",
    "        list2.append(j * 68 + i)\n",
    "\n",
    "    for i in range (42, 52):\n",
    "        list2.append(j * 68 + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9ab72-bda5-4a75-9afb-f182fa129e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain1final_g2.shape, xTrain1final_g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615a717-3cbe-4010-b61a-7daf2a107ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain1 = xTrain1final_g2[list1, 12-rolling_window1:, :].get()#/ data_max_mag * data_max\n",
    "yTrain1 = yTrain1final_g2[list1, 12:, :].get()#/ data_max_mag * data_max\n",
    "xVal1 = xVal1final_g1[list2,12-rolling_window1:, :].get()#/ data_max_mag * data_max\n",
    "yVal1 = yVal1final_g1[list2, 12:, :].get()#/ data_max_mag * data_max\n",
    "\n",
    "print(xTrain1.shape,yTrain1.shape, xVal1.shape, yVal1.shape)\n",
    "yVal1true_Original1.shape, yTrain1true_Original1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0342f1-c764-4081-96ff-1ce251a7b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_x_train = xTrain1.copy()\n",
    "augmented_y_train = yTrain1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810513df-9f15-4093-ae68-998889a4d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warping(sequence, factor_range=(0.8, 1.2)):\n",
    "    warp_factor = np.random.uniform(factor_range[0], factor_range[1])\n",
    "    num_points = sequence.shape[0]\n",
    "    warped_indices = np.arange(0, num_points, warp_factor)\n",
    "    warped_indices = np.clip(warped_indices, 0, num_points - 1).astype(int)\n",
    "    return sequence[warped_indices]\n",
    "\n",
    "# Double the data by applying time warping augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ecdc6-b628-4dfd-b9f8-73cbf0f71768",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = xTrain1.shape[0]\n",
    "num_output_elements = 4  # Assuming your output sequences have 4 elements each\n",
    "\n",
    "# Initialize all weights to 1\n",
    "weights = np.ones((num_sequences, num_output_elements))\n",
    "\n",
    "# Define the prioritized ranges\n",
    "prioritized_ranges_morning = []\n",
    "prioritized_ranges_evening = []\n",
    "for i in range(0, num_sequences, 68):\n",
    "    range1 = list(range(i + 8, i + 17))  # Prioritize 8th to 16th sequence in each range\n",
    "    range2 = list(range(i + 40, i + 54))  # Prioritize 42nd to 52nd sequence in each range\n",
    "    prioritized_ranges_morning.extend(range1)\n",
    "    prioritized_ranges_evening.extend(range2)\n",
    "'''\n",
    "prioritized_ranges_morning = [num for num in prioritized_ranges_morning if num <= len(xTrain1)]\n",
    "prioritized_ranges_evening = [num for num in prioritized_ranges_evening if num <= len(xTrain1)]\n",
    "# Assign higher weights to the prioritized sequences for each output element\n",
    "weights[prioritized_ranges_morning, :] = 1\n",
    "# Assign higher weights to the prioritized sequences for each output element\n",
    "weights[prioritized_ranges_evening, :] = 1\n",
    "'''\n",
    "weights_tf = tf.constant(weights, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cdd09-9161-4799-962e-45657c82c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "xVal1.shape[0]/35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462525fa-9fbb-45ed-a330-dd8e68789bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_periods = yTrain1.shape[1]\n",
    "num_spatial_features = yTrain1.shape[2]\n",
    "weights_tf = tf.tile(weights[:, :, tf.newaxis], (1, 1, num_spatial_features))  # Shape: (1156, 4, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa85ce9-ca8d-4cfe-a7f9-ce6f45e10265",
   "metadata": {},
   "source": [
    "def custom_loss3(weights_tf):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        tf.print(weights_tf.shape, y_true.shape, y_pred.shape)\n",
    "        squared_diff = tf.square(y_true - y_pred)\n",
    "        batch_indices = tf.range(batchs)  # Define batch indices\n",
    "        #tf.print(batch_indices, batchs)\n",
    "        # Select the corresponding weights for the batch\n",
    "        tiled_batch_weights = tf.gather(weights_tf, batch_indices)  # Shape: (batch_size, 10, 4)\n",
    "        '''tf.print(tf.reduce_sum(tiled_batch_weights), \n",
    "                 tiled_batch_weights.shape, \n",
    "                 tf.reduce_max(tiled_batch_weights))'''\n",
    "        tiled_batch_weights_float32 = tf.cast(tiled_batch_weights, dtype=tf.float32)\n",
    "        #tf.print('tiled_batch_weights', tiled_batch_weights.shape)\n",
    "        #tf.print('tiled_batch_weights_float32', tiled_batch_weights_float32.shape)\n",
    "        squared_diff_float32 = tf.cast(squared_diff, dtype=tf.float32)\n",
    "        #tf.print('squared_diff_float32', squared_diff_float32.shape)\n",
    "        #tf.print(squared_diff_float32, tiled_batch_weights_float32)\n",
    "        weighted_squared_diff = squared_diff_float32 * tiled_batch_weights_float32\n",
    "        return weighted_squared_diff\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a72a3-ac6c-464a-8808-50072fbd20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_transposed = tf.transpose(tf.convert_to_tensor(yTrain1, dtype=tf.float32), [2, 1, 0])\n",
    "output_array_pred = tf.reduce_sum(y_pred_transposed, axis=1)\n",
    "#data_max * tf.matmul(A_tensor, tf.matmul(mag_A_inverse_tensor, tf.convert_to_tensor(output_array_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6273c9c-bf13-42e3-a260-d2c9897dd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f7538-7ecd-4240-b01d-61611d81a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tensor = tf.convert_to_tensor(A, dtype=tf.float32)\n",
    "mag_A_inverse_tensor = tf.convert_to_tensor(mag_A_inverse, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def custom_loss5(weights_tf, A_tensor, mag_A_inverse_tensor, data_max):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        #constant = 1/ (weights_tf.shape[0] * weights_tf.shape[2])\n",
    "        #tf.print(weights_tf.shape, y_true.shape, y_pred.shape)\n",
    "        y_pred_transposed = tf.transpose(y_pred, [2, 1, 0])\n",
    "        y_pred_transposed = tf.maximum(y_pred_transposed, 0.0)\n",
    "        output_array_pred = tf.reduce_sum(y_pred_transposed, axis=1)\n",
    "        dot_product_ypred =  data_max * tf.matmul(A_tensor,\n",
    "                                                  tf.matmul(mag_A_inverse_tensor, tf.convert_to_tensor(output_array_pred)))\n",
    "\n",
    "\n",
    "        y_true_transposed = tf.transpose(y_true, [2, 1, 0])\n",
    "        output_array_true = tf.reduce_sum(y_true_transposed, axis=1)\n",
    "        dot_product_ytrue =  data_max * tf.matmul(A_tensor, \n",
    "                                                  tf.matmul(mag_A_inverse_tensor, tf.convert_to_tensor(output_array_true)))\n",
    "        #tf.print(dot_product_ytrue)\n",
    "        #tf.print(dot_product_ypred)\n",
    "        \n",
    "        neum = 2* tf.square(dot_product_ytrue - dot_product_ypred) + 1\n",
    "        denom = dot_product_ytrue + tf.abs(dot_product_ypred) + 1\n",
    "        \n",
    "        squared_difference = tf.math.sqrt(neum / denom)\n",
    "        #tf.print('neum' , neum, 'denom', denom, 'squared_difference', squared_difference)\n",
    "        five_minus_V = 5- squared_difference\n",
    "        five_minus_V = tf.where(tf.equal(five_minus_V, 0), 0.001, five_minus_V)\n",
    "        abs_five_minus_V = abs(five_minus_V)\n",
    "        final = 0.5 -  0.5*(five_minus_V)/(abs_five_minus_V)\n",
    "        #tf.print(constant * tf.reduce_max(final))\n",
    "        #tf.print(squared_difference,  tf.reduce_max(squared_difference), tf.reduce_sum(final))\n",
    "        squared_difference2 = tf.square(y_true - y_pred)\n",
    "        #tf.print(tf.reduce_sum( squared_difference2) , tf.reduce_sum(final))\n",
    "        return  tf.reduce_sum( squared_difference2)  #tf.reduce_sum(final) #+ 0.0001* tf.reduce_sum( squared_difference2) \n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589e401-c2a8-423a-bb8e-7c8929a47e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tensor = tf.convert_to_tensor(A, dtype=tf.float32)\n",
    "mag_A_inverse_tensor = tf.convert_to_tensor(mag_A_inverse, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def custom_loss5(weights_tf, A_tensor, mag_A_inverse_tensor, data_max):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        #constant = 1/ (weights_tf.shape[0] * weights_tf.shape[2])\n",
    "        #tf.print(weights_tf.shape, y_true.shape, y_pred.shape)\n",
    "        y_pred_transposed = tf.transpose(y_pred, [2, 1, 0])\n",
    "        y_pred_transposed = tf.maximum(y_pred_transposed, 0.0)\n",
    "        output_array_pred = tf.reduce_sum(y_pred_transposed, axis=1)\n",
    "\n",
    "        y_true_transposed = tf.transpose(y_true, [2, 1, 0])\n",
    "        output_array_true = tf.reduce_sum(y_true_transposed, axis=1)\n",
    "\n",
    "        squared_difference2 = tf.square(y_true - y_pred)\n",
    "        difference_hourly2 = tf.square(output_array_true - output_array_pred)\n",
    "        #tf.print(tf.reduce_sum( squared_difference2) , tf.reduce_sum(final))\n",
    "        return  tf.reduce_sum( squared_difference2) + 0.00001* tf.reduce_sum( difference_hourly2) #tf.reduce_sum(final) #+ 0.0001* tf.reduce_sum( squared_difference2) \n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30377f0-0c51-4136-8e75-0b7991f86ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(alpha = 0.1):\n",
    "    def my_loss_fn_GEH_percent(y_true, y_pred):\n",
    "        squared_difference = tf.math.sqrt(2 * tf.square(4*y_true - 4*y_pred) + 0.000000001)/(4* y_true + tf.abs(4*y_pred) +  0.000000001)\n",
    "        five_minus_V = 5- squared_difference\n",
    "        abs_five_minus_V = abs(5- squared_difference)\n",
    "        final = 0.5 +  0.5*five_minus_V/abs_five_minus_V\n",
    "        squared_difference2 = tf.square(y_true - y_pred)\n",
    "        \n",
    "        return tf.reduce_sum(0* final + squared_difference2)\n",
    "    return my_loss_fn_GEH_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67a2c6-7b34-4c62-93c2-9b83bdc09b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb797b7-bd7b-4124-9ef1-9fbb69bcf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affc995-7a56-4c94-8309-cf9d242b2504",
   "metadata": {},
   "source": [
    "def bilstm_attention_new1(input_shape,bi_layers,optim, lstm_units,lstm_act,\n",
    "                     return_seq,lstm_kernel_type,dropout_percent,\n",
    "                     dense_units,dense_act,loss_metrics, forecast_window, rolling_window1, custom_loss3, weights_tf):\n",
    "    print('yes')\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    tf.print(\"Shape of input_layer:\", input_layer.shape)\n",
    "    layer = Reshape((rolling_window1, input_shape[0] *input_shape[2]  ))(input_layer)\n",
    "    '''layer = Flatten(input_shape=(input_shape[0], rolling_window1, input_shape[2]))(input_layer)\n",
    "    tf.print('Flatten', layer.shape)\n",
    "    layer = Dense(rolling_window1 * input_shape[-1], activation='elu',\n",
    "                    bias_regularizer=regularizers.L2(.1), \n",
    "                    kernel_regularizer=regularizers.L2(.1),)(layer)\n",
    "    tf.print('Dense', layer.shape)\n",
    "    layer = Reshape((rolling_window1, input_shape[-1]))(layer)\n",
    "    tf.print('Rehape', layer.shape)'''\n",
    "    layer = TimeDistributed(tf.keras.layers.Flatten())(layer)\n",
    "    tf.print('flattened layer', layer.shape)\n",
    "    for i in range(0, bi_layers):\n",
    "        layer = Bidirectional(LSTM(lstm_units,activation=lstm_act,return_sequences=return_seq,\n",
    "                                   kernel_initializer=lstm_kernel_type,\n",
    "                                   bias_regularizer=regularizers.L2(.00001), \n",
    "                                   kernel_regularizer=regularizers.L2(.00001),))(layer)\n",
    "        print(\"Shape of BILSTM_layer:\", layer.shape)\n",
    "        attention_layer = attention()(layer)\n",
    "        layer = tf.keras.layers.Dropout(dropout_percent)(layer)\n",
    "    #layer 3\n",
    "    flat_layer = Flatten()(layer)\n",
    "    print(\"Shape of flat_layer:\", layer.shape)\n",
    "    reshape_layer = Reshape((forecast_window,2*(lstm_units*(int(rolling_window1/forecast_window)))))(flat_layer)\n",
    "    print(\"Shape of reshaped_layer:\", reshape_layer.shape)\n",
    "    print('dense_units', dense_units)\n",
    "    layer = Dense(dense_units,activation = dense_act)(reshape_layer)\n",
    "    print('dense_layer', layer.shape)\n",
    "    output_layer = Dense(input_shape[2],bias_regularizer=regularizers.L2(.01))(layer)\n",
    "    print('output_layer', layer.shape)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    #optim = tf.keras.optimizers.Adam(learning_rate=lr_schedule(step),decay = 0,clipnorm=None)\n",
    "    model.compile(loss=custom_loss3(weights_tf = weights_tf),metrics=loss_metrics, optimizer= optim)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa414e-4c48-4982-8885-387d618ea819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "class SequenceDataGenerator(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size, data_augmentation=True):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.data_augmentation = data_augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x_data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = self.x_data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_y = self.y_data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            # Apply your data augmentation transformations here\n",
    "            pass\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dc0f4-8483-4722-aa3c-26a6cab9c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = SequenceDataGenerator(xTrain1, yTrain1, batch_size=batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b46b0-bb69-4443-92ce-379bff31c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8d99b-7b96-4f31-9844-8a9f5e9c72e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "input_shape=(xTrain1.shape[1],xTrain1.shape[2])\n",
    "print(xTrain1.shape[1],yTrain1.shape[2])\n",
    "model_bilstm2 = bilstm_attention(input_shape,bi_layers,optim, lstm_units,lstm_act,\n",
    "                                return_seq,lstm_kernel_type,dropout_percent,\n",
    "                                dense_units,dense_act,loss_metrics, forecast_window, rolling_window1,\n",
    "                                custom_loss5, weights_tf, A_tensor, data_max, mag_A_inverse_tensor)\n",
    "model_history = model_bilstm2.fit(xTrain1,yTrain1,\n",
    "                           validation_data = (xVal1,yVal1),\n",
    "                           epochs=Epochs,callbacks=clbcks,batch_size=batchs,shuffle=True)\n",
    "print('Total Model Training Time >> ',sum(cb.logs)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50431db3-4395-41b2-862b-234e7c1df987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bilstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb82dcf-b522-4804-a046-01f5bab0be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "#batchs = 68*270\n",
    "Epochs = 100\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "lr_metric = get_lr_metric(optim)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      mode='min', \n",
    "                                      patience=500)\n",
    "cb = TimingCallback()\n",
    "clbcks = [es,cb, PlotLossesKeras(from_step=1)]\n",
    "loss_metrics = [RootMeanSquaredError(name='rmse'),\n",
    "                MeanAbsolutePercentageError(name='mape'),\n",
    "                get_lr_metric(optim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fe947-d5b9-47ce-8f4f-cc3164388f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_bilstm2.compile(loss=custom_loss5(weights_tf = weights_tf, A_tensor = A_tensor,data_max = data_max, mag_A_inverse_tensor = mag_A_inverse_tensor),\n",
    "                      metrics=loss_metrics, \n",
    "                      optimizer= optim)\n",
    "model_history = model_bilstm2.fit(xTrain1,yTrain1,\n",
    "                           validation_data = (xVal1,yVal1),\n",
    "                           epochs=Epochs,callbacks = clbcks,\n",
    "                                  batch_size=batchs,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a4016-5671-41be-ae1f-d0c75bd5925e",
   "metadata": {},
   "source": [
    "1. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed53456-4357-455d-b810-0657944f7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_final = Index_final.tolist()\n",
    "list1, len(Index_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549495e-282d-434b-9233-e18e00e049db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#xTrain1_Original =xTrain1true_Original1[list1, :, :]\n",
    "yTrain1_Original = yTrain1true_Original1[list1, :, :]\n",
    "#xTrain1_Original  = xTrain1true_Original1 [list1, :, :]\n",
    "yTrain1_Original  = yTrain1true_Original1 [list1, :, :]\n",
    "#multiindex = pd.MultiIndex.from_tuples(Index_final[])\n",
    "multiindex = pd.MultiIndex.from_tuples([Index_final[position] for position in list1])\n",
    "multiindex = multiindex\n",
    "train_pred = model_bilstm2.predict(xTrain1)\n",
    "train_truth = yTrain1\n",
    "#df_true = pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(yTrain1, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df_pred = pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(train_pred, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df = df_pred.copy(deep = True)\n",
    "sums_df_pred = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_pred.columns = multiindex\n",
    "sums_df_pred[sums_df_pred<0] = 0\n",
    "df_rec= pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(yTrain1, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df = df_rec.copy(deep = True)\n",
    "sums_df_rec = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_rec.columns = multiindex\n",
    "sums_df_rec[sums_df_rec<0] = 0\n",
    "df_true_original = pd.DataFrame(unfolding_3D(yTrain1true_Original1.get()[list1, :, :], unfol_dim = 2, other_dim_seq = [0,1] ))\n",
    "df = df_true_original.copy()\n",
    "sums_df_Original = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_Original.columns = multiindex\n",
    "GEH_df = np.sqrt((2* np.power(sums_df_Original - sums_df_pred, 2) + 0.00000001)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print((np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001)).isna().any().any())\n",
    "gedf = (np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print(sums_df_pred[sums_df_pred<0].any().any())\n",
    "print((GEH_df[GEH_df<=5] / GEH_df[GEH_df<=5]).sum().sum() / (GEH_df.shape[0]* GEH_df.shape[1]) * 100)\n",
    "GEH_df_time_of_day = GEH_df.T.unstack(0).T\n",
    "less_than_5_count = (GEH_df_time_of_day <= 5).sum(axis = 0)\n",
    "# Calculate the total number of values in the 'Column1' column\n",
    "total_count = GEH_df_time_of_day.count(axis = 0)\n",
    "# Compute the percentage of values less than 5\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.plot()\n",
    "print('Morning average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[28:36].mean()[0])\n",
    "print('Evening average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[62:72].mean()[0])\n",
    "\n",
    "print('Motorways')\n",
    "sums_df_Original = sums_df_Original.T\n",
    "sums_df_Original.columns = train_data_shubham.columns\n",
    "sums_df_pred = sums_df_pred.T\n",
    "sums_df_pred.columns = train_data_shubham.columns\n",
    "sums_df_pred = sums_df_pred[MWY_Sections['Sections'].to_list()].T\n",
    "sums_df_Original = sums_df_Original[MWY_Sections['Sections'].to_list()].T\n",
    "GEH_df = np.sqrt((2* np.power(sums_df_Original - sums_df_pred, 2) + 0.00000001)/ (sums_df_Original \n",
    "                                                                                  + sums_df_pred + 0.00000001))\n",
    "print((np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001)).isna().any().any())\n",
    "gedf = (np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print(sums_df_pred[sums_df_pred<0].any().any())\n",
    "print((GEH_df[GEH_df<=5] / GEH_df[GEH_df<=5]).sum().sum() / (GEH_df.shape[0]* GEH_df.shape[1]) * 100)\n",
    "GEH_df_time_of_day = GEH_df.T.unstack(0).T\n",
    "less_than_5_count = (GEH_df_time_of_day <= 5).sum(axis = 0)\n",
    "# Calculate the total number of values in the 'Column1' column\n",
    "total_count = GEH_df_time_of_day.count(axis = 0)\n",
    "# Compute the percentage of values less than 5\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.plot()\n",
    "print('Morning average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[28:36].mean()[0])\n",
    "print('Evening average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[62:72].mean()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa461e38-c504-486d-a934-bb6eaae1b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xTrain1_Original =xTrain1true_Original1[list2, :, :]\n",
    "yTrain1_Original = yTrain1true_Original1[list2, :, :]\n",
    "#xTrain1_Original  = xTrain1true_Original1 [list2, :, :]\n",
    "yTrain1_Original  = yTrain1true_Original1 [list2, :, :]\n",
    "#multiindex = pd.MultiIndex.from_tuples(Index_final[])\n",
    "multiindex = pd.MultiIndex.from_tuples([Index_final[position] for position in list2])\n",
    "multiindex = multiindex\n",
    "train_pred = model_bilstm2.predict(xVal1)\n",
    "train_truth = yTrain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3808e4-e1d8-486b-a705-3ca61b0aca4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_true = pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(yTrain1, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df_pred = pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(train_pred, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df = df_pred.copy(deep = True)\n",
    "sums_df_pred = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_pred.columns = multiindex\n",
    "sums_df_pred[sums_df_pred<0] = 0\n",
    "df_rec= pd.DataFrame(A.dot(mag_A_inverse.dot(unfolding_3D(yVal1, unfol_dim = 2, other_dim_seq = [0,1] ))))*data_max #/data_max * data_max_mag \n",
    "df = df_rec.copy(deep = True)\n",
    "sums_df_rec = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_rec.columns = multiindex\n",
    "sums_df_rec[sums_df_rec<0] = 0\n",
    "df_true_original = pd.DataFrame(unfolding_3D(yVal1true_Original1.get()[list2, :, :], unfol_dim = 2, other_dim_seq = [0,1] ))\n",
    "df = df_true_original.copy()\n",
    "sums_df_Original = pd.DataFrame({\n",
    "    f'Sum_{i//4+1}': df.iloc[:, i:i+4].sum(axis=1)\n",
    "    for i in range(0, len(df.columns), 4)\n",
    "})\n",
    "# Set the MultiIndex as the column names of the sums_df DataFrame\n",
    "sums_df_Original.columns = multiindex\n",
    "GEH_df = np.sqrt((2* np.power(sums_df_Original - sums_df_pred, 2) + 0.00000001)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print((np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001)).isna().any().any())\n",
    "gedf = (np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print(sums_df_pred[sums_df_pred<0].any().any())\n",
    "print((GEH_df[GEH_df<=5] / GEH_df[GEH_df<=5]).sum().sum() / (GEH_df.shape[0]* GEH_df.shape[1]) * 100)\n",
    "GEH_df_time_of_day = GEH_df.T.unstack(0).T\n",
    "less_than_5_count = (GEH_df_time_of_day <= 5).sum(axis = 0)\n",
    "# Calculate the total number of values in the 'Column1' column\n",
    "total_count = GEH_df_time_of_day.count(axis = 0)\n",
    "# Compute the percentage of values less than 5\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.plot()\n",
    "print('Morning average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[28:36].mean()[0])\n",
    "print('Evening average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[62:72].mean()[0])\n",
    "\n",
    "print('Motorways')\n",
    "sums_df_Original = sums_df_Original.T\n",
    "sums_df_Original.columns = train_data_shubham.columns\n",
    "sums_df_pred = sums_df_pred.T\n",
    "sums_df_pred.columns = train_data_shubham.columns\n",
    "sums_df_pred = sums_df_pred[MWY_Sections['Sections'].to_list()].T\n",
    "sums_df_Original = sums_df_Original[MWY_Sections['Sections'].to_list()].T\n",
    "sums_df_rec = sums_df_rec.T\n",
    "sums_df_rec.columns = train_data_shubham.columns\n",
    "sums_df_rec = sums_df_rec[MWY_Sections['Sections'].to_list()].T\n",
    "GEH_df = np.sqrt((2* np.power(sums_df_Original - sums_df_pred, 2) + 0.00000001)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print((np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001)).isna().any().any())\n",
    "gedf = (np.power(sums_df_Original - sums_df_pred, 2)/ (sums_df_Original + sums_df_pred + 0.00000001))\n",
    "print(sums_df_pred[sums_df_pred<0].any().any())\n",
    "print((GEH_df[GEH_df<=5] / GEH_df[GEH_df<=5]).sum().sum() / (GEH_df.shape[0]* GEH_df.shape[1]) * 100)\n",
    "GEH_df_time_of_day = GEH_df.T.unstack(0).T\n",
    "less_than_5_count = (GEH_df_time_of_day <= 5).sum(axis = 0)\n",
    "# Calculate the total number of values in the 'Column1' column\n",
    "total_count = GEH_df_time_of_day.count(axis = 0)\n",
    "# Compute the percentage of values less than 5\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.plot()\n",
    "print('Morning average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[28:36].mean()[0])\n",
    "print('Evening average' , pd.DataFrame(percentage_less_than_5_time_of_day).loc[64:72].mean()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829fba3-7202-4635-bfbc-610b526bccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d0f34-45d0-45f4-81b6-f7bd8ff5e809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GEH_df_time_of_day_eve = GEH_df_time_of_day.T[44:52].T.unstack(1)\n",
    "less_than_5_count = (GEH_df_time_of_day_eve <= 5).sum(axis = 1)\n",
    "total_count = GEH_df_time_of_day_eve.count(axis = 1)\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b4d74-9f3b-4e65-84ea-f9560556fcdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GEH_df_time_of_day_eve = GEH_df_time_of_day.T[44:52].T.unstack(0)\n",
    "less_than_5_count = (GEH_df_time_of_day_eve <= 5).sum(axis = 1)\n",
    "total_count = GEH_df_time_of_day_eve.count(axis = 1)\n",
    "percentage_less_than_5_time_of_day = (less_than_5_count / total_count) * 100\n",
    "percentage_less_than_5_time_of_day.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4117fdd-7894-44f5-9262-671046e7ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GEH_df_time_of_day_eve.copy(deep = True)\n",
    "# Define the bins\n",
    "bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, float('inf')]\n",
    "\n",
    "# Calculate the percentage of values in each bin\n",
    "bin_counts = pd.cut(df.unstack(1), bins=bins, right=False).value_counts(sort=False)\n",
    "total_count = len(df.unstack(1))\n",
    "\n",
    "percentage_values = (bin_counts / total_count) * 100\n",
    "\n",
    "# Print the results\n",
    "for bin_range, percentage in zip(bin_counts.index, percentage_values):\n",
    "    print(f'Range {bin_range}: {percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd952e78-2657-4233-94db-1ecf8257e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "22.74+20.11+16.20+13.20+9.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804939d-dff6-42ba-a4c7-c3a7a1ddd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pred.copy(deep = True)\n",
    "valid_times1 = ['00:00:00', '00:15:00', '00:30:00', '00:45:00', '01:00:00', '01:15:00',\n",
    "       '01:30:00', '01:45:00', '02:00:00', '02:15:00', '02:30:00', '02:45:00',\n",
    "       '03:00:00', '03:15:00', '03:30:00', '03:45:00', '04:00:00', '04:15:00',\n",
    "       '04:30:00',  '21:45:00', '22:00:00', '22:15:00',\n",
    "       '22:30:00', '22:45:00', '23:00:00', '23:15:00', '23:30:00', '23:45:00']\n",
    "valid_times2 = ['00:00:00', '00:15:00', '00:30:00', '00:45:00', '01:00:00', '01:15:00',\n",
    "       '01:30:00', '01:45:00', '02:00:00', '02:15:00', '02:30:00', '02:45:00',\n",
    "       '03:00:00', '03:15:00', '03:30:00', '03:45:00', '04:00:00', '04:15:00',\n",
    "       '04:30:00',  '21:45:00', '22:00:00', '22:15:00',\n",
    "       '22:30:00', '22:45:00', '23:00:00', '23:15:00', '23:30:00', '23:45:00']\n",
    "valid_times3 = ['00:00:00', '00:15:00', '00:30:00', '00:45:00', '01:00:00', '01:15:00',\n",
    "       '01:30:00', '01:45:00', '02:00:00', '02:15:00', '02:30:00', '02:45:00',\n",
    "       '03:00:00', '03:15:00', '03:30:00', '03:45:00', '04:00:00', '04:15:00',\n",
    "       '04:30:00',  '21:45:00', '22:00:00', '22:15:00',\n",
    "       '22:30:00', '22:45:00', '23:00:00', '23:15:00', '23:30:00', '23:45:00']\n",
    "valid_times4 = ['00:00:00', '00:15:00', '00:30:00', '00:45:00', '01:00:00', '01:15:00',\n",
    "       '01:30:00', '01:45:00', '02:00:00', '02:15:00', '02:30:00', '02:45:00',\n",
    "       '03:00:00', '03:15:00', '03:30:00', '03:45:00', '04:00:00', '04:15:00',\n",
    "       '04:30:00',  '21:45:00', '22:00:00', '22:15:00',\n",
    "       '22:30:00', '22:45:00', '23:00:00', '23:15:00', '23:30:00', '23:45:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473b05a-9872-4405-9b10-a6f4a1f4eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec0009-e0e8-44cb-8db8-07751e54739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pred.copy(deep = True)\n",
    "dataframes = []\n",
    "for i in range(4):\n",
    "    selected_cols = df.iloc[:, i::4]\n",
    "    dataframes.append(selected_cols)\n",
    "# Rename the columns of the split DataFrames\n",
    "for i, selected_df in enumerate(dataframes):\n",
    "    selected_df.columns = [f'col{i*4+j}' for j in range(selected_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3f35d-acec-4223-bf91-37aa63ea7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5f143-8465-4e17-8f61-10c07bf4ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_true_original.copy(deep = True)\n",
    "dataframes_true = []\n",
    "for i in range(4):\n",
    "    selected_cols = df.iloc[:, i::4]\n",
    "    dataframes_true.append(selected_cols)\n",
    "\n",
    "# Rename the columns of the split DataFrames\n",
    "for i, selected_df in enumerate(dataframes_true):\n",
    "    selected_df.columns = [f'col{i*4+j}' for j in range(selected_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507a0ca-85c0-4fe4-b6d4-ca1a09211eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_rec.copy(deep = True)\n",
    "dataframes_rec = []\n",
    "for i in range(4):\n",
    "    selected_cols = df.iloc[:, i::4]\n",
    "    dataframes_rec.append(selected_cols)\n",
    "\n",
    "# Rename the columns of the split DataFrames\n",
    "for i, selected_df in enumerate(dataframes_rec):\n",
    "    selected_df.columns = [f'col{i*4+j}' for j in range(selected_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3bc12-7739-4930-82b0-965f108475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test_data_prior.T.unstack(0)\n",
    "test1 = test1[~test1.index.isin(valid_times1)].T.unstack(1).T.unstack(0).T.unstack(1)\n",
    "\n",
    "test2 = test_data_prior.T.unstack(0)\n",
    "test2 = test2[~test2.index.isin(valid_times1)].T.unstack(1).T.unstack(0).T.unstack(1)\n",
    "\n",
    "test3 = test_data_prior.T.unstack(0)\n",
    "test3 = test3[~test3.index.isin(valid_times1)].T.unstack(1).T.unstack(0).T.unstack(1)\n",
    "\n",
    "test4 = test_data_prior.T.unstack(0)\n",
    "test4 = test4[~test4.index.isin(valid_times1)].T.unstack(1).T.unstack(0).T.unstack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ba256-6cbb-4b13-8df6-7f978f8276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prior.T.unstack(0)[20+42:20+52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43187027-3481-4c5e-b4a7-8d29f6d26f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0].index = test1.index\n",
    "dataframes[0].columns = test1.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes[1].index = test2.index\n",
    "dataframes[1].columns = test2.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes[2].index = test3.index\n",
    "dataframes[2].columns = test3.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes[3].index = test4.index\n",
    "dataframes[3].columns = test4.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "\n",
    "dataframes_true[0].index = test1.index\n",
    "dataframes_true[0].columns = test1.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_true[1].index = test2.index\n",
    "dataframes_true[1].columns = test2.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_true[2].index = test3.index\n",
    "dataframes_true[2].columns = test3.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_true[3].index = test4.index\n",
    "dataframes_true[3].columns = test4.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_rec[0].index = test1.index\n",
    "dataframes_rec[0].columns = test1.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_rec[1].index = test2.index\n",
    "dataframes_rec[1].columns = test2.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_rec[2].index = test3.index\n",
    "dataframes_rec[2].columns = test3.columns[:dataframes[0].columns.shape[0], ]\n",
    "\n",
    "dataframes_rec[3].index = test4.index\n",
    "dataframes_rec[3].columns = test4.columns[:dataframes[0].columns.shape[0], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f62e63-fb6e-49e0-bec8-867c9d179476",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bef461-08cb-4282-9487-8765c899f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0*68 \n",
    "b = 3*68\n",
    "Link = '16671173_st0'\n",
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "dataframes[0].T[Link].iloc[a:b].plot(color = 'green')\n",
    "dataframes_true[0].T[Link].iloc[a:b].plot(color = 'red')\n",
    "dataframes_rec[0].T[Link].iloc[a:b].plot(color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e453eb-a7ce-4a90-a743-6437240b568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0*68 \n",
    "b = 10*68\n",
    "Link = '16671173_st0'\n",
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "dataframes[1].T[Link].iloc[a:b].plot(color = 'green')\n",
    "dataframes_true[1].T[Link].iloc[a:b].plot(color = 'red')\n",
    "dataframes_rec[1].T[Link].iloc[a:b].plot(color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3bf1d-892e-457b-8e5d-266f4cdc06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0*68 \n",
    "b = 5*68\n",
    "Link = '16671173_st0'\n",
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "dataframes[2].T[Link].iloc[a:b].plot(color = 'green')\n",
    "dataframes_true[2].T[Link].iloc[a:b].plot(color = 'red')\n",
    "dataframes_rec[2].T[Link].iloc[a:b].plot(color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9d0fc-7493-4715-8eb3-205dba92368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0*68 \n",
    "b = 5*68\n",
    "Link = '16670948_st0'\n",
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "dataframes[3].T[Link].iloc[a:b].plot(color = 'green')\n",
    "dataframes_true[3].T[Link].iloc[a:b].plot(color = 'red')\n",
    "dataframes_rec[3].T[Link].iloc[a:b].plot(color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede4c77-bc2d-44cc-a9e9-bf7536c38e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse_mape(true_df,pred_df, plot = False ):\n",
    "    rmse = np.sqrt(np.power(true_df - pred_df, 2).sum().sum() / (true_df.shape[0] * true_df.shape[1]))\n",
    "    rmse_rows = np.sqrt(np.power(true_df.T.unstack(0) - pred_df.T.unstack(0), 2).sum(axis = 1) / (pred_df.T.unstack(0).shape[1]))\n",
    "    if plot == True:\n",
    "        rmse_rows.plot()\n",
    "        plt.show()\n",
    "    W = (true_df/true_df).replace(np.nan, 0).replace(np.inf, 0)\n",
    "    MAPE = (abs(true_df - pred_df)/ true_df).replace(np.nan, 0).replace(np.inf, 0).sum().sum() / W.sum().sum() * 100\n",
    "    MAPE_rows = (abs(true_df.T.unstack(0) - pred_df.T.unstack(0) )/ true_df.T.unstack(0) ).replace(np.nan, 0).replace(np.inf, 0).sum(axis = 1) / W.T.unstack(0).sum(axis = 1) * 100\n",
    "    if plot == True:\n",
    "        MAPE_rows.plot()\n",
    "        plt.show()\n",
    "    return rmse, rmse_rows, MAPE, MAPE_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99062c-7495-48fb-83a2-b597f80eef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = dataframes[0]\n",
    "true_df = dataframes_true[0]\n",
    "rmse, rmse_rows, MAPE, MAPE_rows = calculate_rmse_mape(true_df, pred_df)\n",
    "rmse_m, rmse_rows_m, MAPE_m, MAPE_rows_m = calculate_rmse_mape(true_df[true_df.index.isin(MWY_Sections['Sections'].to_list())],\n",
    "                                                               pred_df[true_df.index.isin(MWY_Sections['Sections'].to_list())])\n",
    "print('15 min ahead', rmse, MAPE)\n",
    "print('15 min ahead', rmse_m, MAPE_m)\n",
    "print('Morning stats', MAPE_rows[9:17].mean(), rmse_rows[9:17].mean())\n",
    "print('Evening stats', MAPE_rows[45:54].mean(), rmse_rows[45:54].mean())\n",
    "print('Morning stats Motorways', rmse_rows_m[9:17].mean(), MAPE_rows_m[9:17].mean(), )\n",
    "print('Evening stats Motorways', rmse_rows_m[45:54].mean(), MAPE_rows_m[45:54].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02878c7c-e665-4c97-b505-550d55d8ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = dataframes[1]\n",
    "true_df = dataframes_true[1]\n",
    "rmse, rmse_rows, MAPE, MAPE_rows = calculate_rmse_mape(true_df, pred_df)\n",
    "rmse_m, rmse_rows_m, MAPE_m, MAPE_rows_m = calculate_rmse_mape(true_df[true_df.index.isin(MWY_Sections['Sections'].to_list())],\n",
    "                                                               pred_df[true_df.index.isin(MWY_Sections['Sections'].to_list())])\n",
    "print('15 min ahead', rmse, MAPE)\n",
    "print('15 min ahead', rmse_m, MAPE_m)\n",
    "print('Morning stats', MAPE_rows[9:17].mean(), rmse_rows[9:17].mean())\n",
    "print('Evening stats', MAPE_rows[45:54].mean(), rmse_rows[45:54].mean())\n",
    "print('Morning stats Motorways', rmse_rows_m[9:17].mean(), MAPE_rows_m[9:17].mean(), )\n",
    "print('Evening stats Motorways', rmse_rows_m[45:54].mean(), MAPE_rows_m[45:54].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a187814-0be2-46dc-ade2-a420f35c0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = dataframes[2]\n",
    "true_df = dataframes_true[2]\n",
    "rmse_m, rmse_rows_m, MAPE_m, MAPE_rows_m = calculate_rmse_mape(true_df[true_df.index.isin(MWY_Sections['Sections'].to_list())],\n",
    "                                                               pred_df[true_df.index.isin(MWY_Sections['Sections'].to_list())])\n",
    "print('15 min ahead', rmse, MAPE)\n",
    "print('15 min ahead', rmse_m, MAPE_m)\n",
    "print('Morning stats', MAPE_rows[9:17].mean(), rmse_rows[9:17].mean())\n",
    "print('Evening stats', MAPE_rows[45:54].mean(), rmse_rows[45:54].mean())\n",
    "print('Morning stats Motorways', rmse_rows_m[9:17].mean(), MAPE_rows_m[9:17].mean(), )\n",
    "print('Evening stats Motorways', rmse_rows_m[45:54].mean(), MAPE_rows_m[45:54].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5bd59-b068-421a-970c-18320349163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = dataframes[3]\n",
    "true_df = dataframes_true[3]\n",
    "rmse_m, rmse_rows_m, MAPE_m, MAPE_rows_m = calculate_rmse_mape(true_df[true_df.index.isin(MWY_Sections['Sections'].to_list())],\n",
    "                                                               pred_df[true_df.index.isin(MWY_Sections['Sections'].to_list())])\n",
    "print('15 min ahead', rmse, MAPE)\n",
    "print('15 min ahead', rmse_m, MAPE_m)\n",
    "print('Morning stats', MAPE_rows[9:17].mean(), rmse_rows[9:17].mean())\n",
    "print('Evening stats', MAPE_rows[45:54].mean(), rmse_rows[45:54].mean())\n",
    "print('Morning stats Motorways', rmse_rows_m[9:17].mean(), MAPE_rows_m[9:17].mean(), )\n",
    "print('Evening stats Motorways', rmse_rows_m[45:54].mean(), MAPE_rows_m[45:54].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d79af6-91df-4844-b818-8e0ad766c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(true_df - pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44280bc-b92b-46b1-a3f2-b07df98eab7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbd109-826e-4d93-b467-2495dbc4ac59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe2ca5-d9ae-4399-8a3c-b71eb7667347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a721156-488d-4a66-8895-d2c8c550c3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90016c44-13df-4dfc-8896-17ce7950834a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
